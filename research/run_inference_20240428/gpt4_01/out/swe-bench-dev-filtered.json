[
    {
        "repo": "sqlfluff/sqlfluff",
        "instance_id": "sqlfluff__sqlfluff-2419",
        "base_commit": "f1dba0e1dd764ae72d67c3d5e1471cf14d3db030",
        "patch": "diff --git a/src/sqlfluff/rules/L060.py b/src/sqlfluff/rules/L060.py\n--- a/src/sqlfluff/rules/L060.py\n+++ b/src/sqlfluff/rules/L060.py\n@@ -59,4 +59,8 @@ def _eval(self, context: RuleContext) -> Optional[LintResult]:\n             ],\n         )\n \n-        return LintResult(context.segment, [fix])\n+        return LintResult(\n+            anchor=context.segment,\n+            fixes=[fix],\n+            description=f\"Use 'COALESCE' instead of '{context.segment.raw_upper}'.\",\n+        )\n",
        "test_patch": "diff --git a/test/rules/std_L060_test.py b/test/rules/std_L060_test.py\nnew file mode 100644\n--- /dev/null\n+++ b/test/rules/std_L060_test.py\n@@ -0,0 +1,12 @@\n+\"\"\"Tests the python routines within L060.\"\"\"\n+import sqlfluff\n+\n+\n+def test__rules__std_L060_raised() -> None:\n+    \"\"\"L060 is raised for use of ``IFNULL`` or ``NVL``.\"\"\"\n+    sql = \"SELECT\\n\\tIFNULL(NULL, 100),\\n\\tNVL(NULL,100);\"\n+    result = sqlfluff.lint(sql, rules=[\"L060\"])\n+\n+    assert len(result) == 2\n+    assert result[0][\"description\"] == \"Use 'COALESCE' instead of 'IFNULL'.\"\n+    assert result[1][\"description\"] == \"Use 'COALESCE' instead of 'NVL'.\"\n",
        "problem_statement": "Rule L060 could give a specific error message\nAt the moment rule L060 flags something like this:\r\n\r\n```\r\nL:  21 | P:   9 | L060 | Use 'COALESCE' instead of 'IFNULL' or 'NVL'.\r\n```\r\n\r\nSince we likely know the wrong word, it might be nice to actually flag that instead of both `IFNULL` and `NVL` - like most of the other rules do.\r\n\r\nThat is it should flag this:\r\n\r\n```\r\nL:  21 | P:   9 | L060 | Use 'COALESCE' instead of 'IFNULL'.\r\n```\r\n Or this:\r\n\r\n```\r\nL:  21 | P:   9 | L060 | Use 'COALESCE' instead of 'NVL'.\r\n```\r\n\r\nAs appropriate.\r\n\r\nWhat do you think @jpy-git ?\r\n\n",
        "hints_text": "@tunetheweb Yeah definitely, should be a pretty quick change \ud83d\ude0a",
        "created_at": "2022-01-22T12:21:52Z",
        "version": "0.8",
        "FAIL_TO_PASS": [
            "test/rules/std_L060_test.py::test__rules__std_L060_raised"
        ],
        "PASS_TO_PASS": [],
        "environment_setup_commit": "a5c4eae4e3e419fe95460c9afd9cf39a35a470c4"
    },
    {
        "repo": "sqlfluff/sqlfluff",
        "instance_id": "sqlfluff__sqlfluff-2998",
        "base_commit": "47c8bb29104761474e455ef2e6fdaa7a8cc20a56",
        "patch": "diff --git a/src/sqlfluff/rules/L027.py b/src/sqlfluff/rules/L027.py\n--- a/src/sqlfluff/rules/L027.py\n+++ b/src/sqlfluff/rules/L027.py\n@@ -73,4 +73,21 @@ def _lint_references_and_aliases(\n                     )\n                 )\n \n+            all_table_aliases = [t.ref_str for t in table_aliases] + standalone_aliases\n+\n+            # For qualified references, we want to check that the alias is actually\n+            # valid\n+            if (\n+                this_ref_type == \"qualified\"\n+                and list(r.iter_raw_references())[0].part not in all_table_aliases\n+            ):\n+                violation_buff.append(\n+                    LintResult(\n+                        anchor=r,\n+                        description=f\"Qualified reference {r.raw!r} not found in \"\n+                        f\"available tables/view aliases {all_table_aliases} in select \"\n+                        \"with more than one referenced table/view.\",\n+                    )\n+                )\n+\n         return violation_buff or None\n",
        "test_patch": "diff --git a/test/fixtures/rules/std_rule_cases/L027.yml b/test/fixtures/rules/std_rule_cases/L027.yml\n--- a/test/fixtures/rules/std_rule_cases/L027.yml\n+++ b/test/fixtures/rules/std_rule_cases/L027.yml\n@@ -220,3 +220,40 @@ test_pass_rowtype_with_join:\n   configs:\n     core:\n       dialect: hive\n+\n+test_fail_column_name_not_found_in_table_aliases_bigquery:\n+  # qualified reference should actually exists in table aliases\n+  fail_str: |\n+    SELECT\n+        a.bar,\n+        b.foo,\n+        this_is.some_struct.id\n+    FROM\n+        a LEFT JOIN b ON TRUE\n+  configs:\n+    core:\n+      dialect: bigquery\n+\n+test_pass_column_name_is_a_struct_bigquery:\n+  # check structs work as expected\n+  pass_str: |\n+    SELECT\n+        a.bar,\n+        b.this_is.some_struct.id\n+    FROM\n+        a LEFT JOIN b ON TRUE\n+  configs:\n+    core:\n+      dialect: bigquery\n+\n+test_pass_column_name_from_unnest_bigquery:\n+  # Check that we allow an table alias come from UNNEST statement\n+  pass_str: |\n+    SELECT\n+        a.bar,\n+        e.foo\n+    FROM\n+        a LEFT JOIN UNEST(a.events) AS e\n+  configs:\n+    core:\n+      dialect: bigquery\ndiff --git a/test/rules/std_test.py b/test/rules/std_test.py\n--- a/test/rules/std_test.py\n+++ b/test/rules/std_test.py\n@@ -68,7 +68,7 @@\n         ),\n         (\"L016\", \"block_comment_errors_2.sql\", [(1, 85), (2, 86)]),\n         # Column references\n-        (\"L027\", \"column_references.sql\", [(1, 8)]),\n+        (\"L027\", \"column_references.sql\", [(1, 8), (1, 11)]),\n         (\"L027\", \"column_references_bare_function.sql\", []),\n         (\"L026\", \"column_references.sql\", [(1, 11)]),\n         (\"L025\", \"column_references.sql\", [(2, 11)]),\n",
        "problem_statement": "BigQuery: Accessing `STRUCT` elements evades triggering L027\n### Search before asking\n\n- [X] I searched the [issues](https://github.com/sqlfluff/sqlfluff/issues) and found no similar issues.\n\n\n### What Happened\n\nAccessing unreferenced `STRUCT` elements using BigQuery dot notation in a multi table query does not trigger L027.\n\n### Expected Behaviour\n\nL027 gets triggered.\n\n### Observed Behaviour\n\nL027 does not get triggered.\n\n### How to reproduce\n\n```sql\r\nSELECT\r\n    t1.col1,\r\n    t2.col2,\r\n    events.id\r\nFROM t_table1 AS t1\r\nLEFT JOIN t_table2 AS t2\r\n    ON TRUE\r\n```\n\n### Dialect\n\nBigQUery\n\n### Version\n\n`0.11.2` using online.sqlfluff.com\n\n### Configuration\n\nN/A\n\n### Are you willing to work on and submit a PR to address the issue?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [X] I agree to follow this project's [Code of Conduct](https://github.com/sqlfluff/sqlfluff/blob/main/CODE_OF_CONDUCT.md)\n\n",
        "hints_text": "This is tricky.\r\n\r\nBasicaly L026 works to make sure qualified columns only use tables in the from clause. This doesn\u2019t really work for `STRUCT`s as impossible to know if it\u2019s a qualified column or a `STRUCT`, so is off by default for languages that support them - like BigQuery.\r\n\r\nL027 works to make sure columns are qualified for multi-table joins (i.e. have at least one dot). But it doesn\u2019t check the qualifiers are valid - that\u2019s L026\u2019s job, which as I say is off by default for BigQuery.",
        "created_at": "2022-04-04T20:29:42Z",
        "version": "0.11",
        "FAIL_TO_PASS": [
            "test/rules/std_test.py::test__rules__std_file[L027-column_references.sql-violations16]"
        ],
        "PASS_TO_PASS": [
            "test/rules/std_test.py::test__rules__std_file[L001-indentation_errors.sql-violations0]",
            "test/rules/std_test.py::test__rules__std_file[L002-indentation_errors.sql-violations1]",
            "test/rules/std_test.py::test__rules__std_file[L003-indentation_errors.sql-violations2]",
            "test/rules/std_test.py::test__rules__std_file[L004-indentation_errors.sql-violations3]",
            "test/rules/std_test.py::test__rules__std_file[L005-whitespace_errors.sql-violations4]",
            "test/rules/std_test.py::test__rules__std_file[L019-whitespace_errors.sql-violations5]",
            "test/rules/std_test.py::test__rules__std_file[L008-whitespace_errors.sql-violations6]",
            "test/rules/std_test.py::test__rules__std_file[L006-operator_errors.sql-violations7]",
            "test/rules/std_test.py::test__rules__std_file[L039-operator_errors.sql-violations8]",
            "test/rules/std_test.py::test__rules__std_file[L007-operator_errors.sql-violations9]",
            "test/rules/std_test.py::test__rules__std_file[L006-operator_errors_negative.sql-violations10]",
            "test/rules/std_test.py::test__rules__std_file[L039-operator_errors_negative.sql-violations11]",
            "test/rules/std_test.py::test__rules__std_file[L003-indentation_error_hard.sql-violations12]",
            "test/rules/std_test.py::test__rules__std_file[L003-indentation_error_contained.sql-violations13]",
            "test/rules/std_test.py::test__rules__std_file[L016-block_comment_errors.sql-violations14]",
            "test/rules/std_test.py::test__rules__std_file[L016-block_comment_errors_2.sql-violations15]",
            "test/rules/std_test.py::test__rules__std_file[L027-column_references_bare_function.sql-violations17]",
            "test/rules/std_test.py::test__rules__std_file[L026-column_references.sql-violations18]",
            "test/rules/std_test.py::test__rules__std_file[L025-column_references.sql-violations19]",
            "test/rules/std_test.py::test__rules__std_file[L021-select_distinct_group_by.sql-violations20]",
            "test/rules/std_test.py::test__rules__std_file[L006-operator_errors_ignore.sql-violations21]",
            "test/rules/std_test.py::test__rules__std_file[L031-aliases_in_join_error.sql-violations22]",
            "test/rules/std_test.py::test__rules__std_file[L046-heavy_templating.sql-violations23]",
            "test/rules/std_test.py::test_improper_configs_are_rejected[rule_config_dict0]",
            "test/rules/std_test.py::test_improper_configs_are_rejected[rule_config_dict1]",
            "test/rules/std_test.py::test_improper_configs_are_rejected[rule_config_dict2]",
            "test/rules/std_test.py::test_improper_configs_are_rejected[rule_config_dict3]",
            "test/rules/std_test.py::test_improper_configs_are_rejected[rule_config_dict4]",
            "test/rules/std_test.py::test_improper_configs_are_rejected[rule_config_dict5]",
            "test/rules/std_test.py::test_improper_configs_are_rejected[rule_config_dict6]",
            "test/rules/std_test.py::test_improper_configs_are_rejected[rule_config_dict7]",
            "test/rules/std_test.py::test_improper_configs_are_rejected[rule_config_dict8]",
            "test/rules/std_test.py::test_improper_configs_are_rejected[rule_config_dict9]",
            "test/rules/std_test.py::test_improper_configs_are_rejected[rule_config_dict10]",
            "test/rules/std_test.py::test_improper_configs_are_rejected[rule_config_dict11]"
        ],
        "environment_setup_commit": "2bdeb9354d33e3fb4dfd6782e1e1921939ecb55a"
    },
    {
        "repo": "sqlfluff/sqlfluff",
        "instance_id": "sqlfluff__sqlfluff-3330",
        "base_commit": "c2b1ec442131a70ac5b1560396ce1bbe636e4864",
        "patch": "diff --git a/src/sqlfluff/rules/L065.py b/src/sqlfluff/rules/L065.py\nnew file mode 100644\n--- /dev/null\n+++ b/src/sqlfluff/rules/L065.py\n@@ -0,0 +1,156 @@\n+\"\"\"Implementation of Rule L065.\"\"\"\n+from typing import List, Optional, Iterable\n+\n+import sqlfluff.core.rules.functional.segment_predicates as sp\n+from sqlfluff.core.parser import BaseSegment, NewlineSegment\n+from sqlfluff.core.rules.base import BaseRule, LintFix, LintResult, RuleContext\n+from sqlfluff.core.rules.doc_decorators import document_fix_compatible, document_groups\n+\n+\n+@document_groups\n+@document_fix_compatible\n+class Rule_L065(BaseRule):\n+    \"\"\"Set operators should be surrounded by newlines.\n+\n+    **Anti-pattern**\n+\n+    In this example, `UNION ALL` is not on a line ifself.\n+\n+    .. code-block:: sql\n+\n+        SELECT 'a' AS col UNION ALL\n+        SELECT 'b' AS col\n+\n+    **Best practice**\n+\n+    .. code-block:: sql\n+\n+        SELECT 'a' AS col\n+        UNION ALL\n+        SELECT 'b' AS col\n+\n+    \"\"\"\n+\n+    groups = (\"all\",)\n+\n+    _target_elems = (\"set_operator\",)\n+\n+    def _eval(self, context: RuleContext) -> List[LintResult]:\n+        \"\"\"Set operators should be surrounded by newlines.\n+\n+        For any set operator we check if there is any NewLineSegment in the non-code\n+        segments preceeding or following it.\n+\n+        In particular, as part of this rule we allow multiple NewLineSegments.\n+        \"\"\"\n+        segment = context.functional.segment\n+\n+        expression = segment.children()\n+        set_operator_segments = segment.children(sp.is_type(*self._target_elems))\n+        results: List[LintResult] = []\n+\n+        # If len(set_operator) == 0 this will essentially not run\n+        for set_operator in set_operator_segments:\n+            preceeding_code = (\n+                expression.reversed().select(start_seg=set_operator).first(sp.is_code())\n+            )\n+            following_code = expression.select(start_seg=set_operator).first(\n+                sp.is_code()\n+            )\n+            res = {\n+                \"before\": expression.select(\n+                    start_seg=preceeding_code.get(), stop_seg=set_operator\n+                ),\n+                \"after\": expression.select(\n+                    start_seg=set_operator, stop_seg=following_code.get()\n+                ),\n+            }\n+\n+            newline_before_set_operator = res[\"before\"].first(sp.is_type(\"newline\"))\n+            newline_after_set_operator = res[\"after\"].first(sp.is_type(\"newline\"))\n+\n+            # If there is a whitespace directly preceeding/following the set operator we\n+            # are replacing it with a newline later.\n+            preceeding_whitespace = res[\"before\"].first(sp.is_type(\"whitespace\")).get()\n+            following_whitespace = res[\"after\"].first(sp.is_type(\"whitespace\")).get()\n+\n+            if newline_before_set_operator and newline_after_set_operator:\n+                continue\n+            elif not newline_before_set_operator and newline_after_set_operator:\n+                results.append(\n+                    LintResult(\n+                        anchor=set_operator,\n+                        description=(\n+                            \"Set operators should be surrounded by newlines. \"\n+                            f\"Missing newline before set operator {set_operator.raw}.\"\n+                        ),\n+                        fixes=_generate_fixes(whitespace_segment=preceeding_whitespace),\n+                    )\n+                )\n+            elif newline_before_set_operator and not newline_after_set_operator:\n+                results.append(\n+                    LintResult(\n+                        anchor=set_operator,\n+                        description=(\n+                            \"Set operators should be surrounded by newlines. \"\n+                            f\"Missing newline after set operator {set_operator.raw}.\"\n+                        ),\n+                        fixes=_generate_fixes(whitespace_segment=following_whitespace),\n+                    )\n+                )\n+            else:\n+                preceeding_whitespace_fixes = _generate_fixes(\n+                    whitespace_segment=preceeding_whitespace\n+                )\n+                following_whitespace_fixes = _generate_fixes(\n+                    whitespace_segment=following_whitespace\n+                )\n+\n+                # make mypy happy\n+                assert isinstance(preceeding_whitespace_fixes, Iterable)\n+                assert isinstance(following_whitespace_fixes, Iterable)\n+\n+                fixes = []\n+                fixes.extend(preceeding_whitespace_fixes)\n+                fixes.extend(following_whitespace_fixes)\n+\n+                results.append(\n+                    LintResult(\n+                        anchor=set_operator,\n+                        description=(\n+                            \"Set operators should be surrounded by newlines. \"\n+                            \"Missing newline before and after set operator \"\n+                            f\"{set_operator.raw}.\"\n+                        ),\n+                        fixes=fixes,\n+                    )\n+                )\n+\n+        return results\n+\n+\n+def _generate_fixes(\n+    whitespace_segment: BaseSegment,\n+) -> Optional[List[LintFix]]:\n+\n+    if whitespace_segment:\n+        return [\n+            LintFix.replace(\n+                anchor_segment=whitespace_segment,\n+                # NB: Currently we are just inserting a Newline here. This alone will\n+                # produce not properly indented SQL. We rely on L003 to deal with\n+                # indentation later.\n+                # As a future improvement we could maybe add WhitespaceSegment( ... )\n+                # here directly.\n+                edit_segments=[NewlineSegment()],\n+            )\n+        ]\n+    else:\n+        # We should rarely reach here as set operators are always surrounded by either\n+        # WhitespaceSegment or NewlineSegment.\n+        # However, in exceptional cases the WhitespaceSegment might be enclosed in the\n+        # surrounding segment hierachy and not accessible by the rule logic.\n+        # At the time of writing this is true for `tsql` as covered in the test\n+        # `test_fail_autofix_in_tsql_disabled`. If we encounter such case, we skip\n+        # fixing.\n+        return []\n",
        "test_patch": "diff --git a/test/fixtures/rules/std_rule_cases/L065.yml b/test/fixtures/rules/std_rule_cases/L065.yml\nnew file mode 100644\n--- /dev/null\n+++ b/test/fixtures/rules/std_rule_cases/L065.yml\n@@ -0,0 +1,122 @@\n+rule: L065\n+\n+test_fail_simple_fix_union_all_before:\n+  fail_str: |\n+      SELECT 'a' UNION ALL\n+      SELECT 'b'\n+  fix_str: |\n+      SELECT 'a'\n+      UNION ALL\n+      SELECT 'b'\n+\n+test_fail_simple_fix_union_all_after:\n+  fail_str: |\n+      SELECT 'a'\n+      UNION ALL SELECT 'b'\n+  fix_str: |\n+      SELECT 'a'\n+      UNION ALL\n+      SELECT 'b'\n+\n+test_fail_simple_fix_union_all_before_and_after:\n+  fail_str: |\n+      SELECT 'a' UNION ALL SELECT 'b'\n+  fix_str: |\n+      SELECT 'a'\n+      UNION ALL\n+      SELECT 'b'\n+\n+test_pass_multiple_newlines_are_allowed:\n+  pass_str: |\n+      SELECT 'a'\n+\n+\n+      UNION ALL\n+\n+\n+      SELECT 'b'\n+\n+# The autofix of L065 doesn't respect indentation of the surrounding query.\n+# Hence, the fix result of only L065 looks ugly. But L003 will fix the indentation\n+# in a second step.\n+# See the test blow.\n+test_fail_fix_works_in_subqueries:\n+  fail_str: |\n+      SELECT * FROM (\n+          SELECT 'g' UNION ALL\n+          SELECT 'h'\n+          UNION ALL SELECT 'j'\n+      )\n+  fix_str: |\n+      SELECT * FROM (\n+          SELECT 'g'\n+      UNION ALL\n+          SELECT 'h'\n+          UNION ALL\n+      SELECT 'j'\n+      )\n+\n+# Test autofix after L003 passes L065\n+test_pass_fix_works_in_subqueries_after_L003_fix:\n+  pass_str: |\n+      SELECT * FROM (\n+          SELECT 'g'\n+          UNION ALL\n+          SELECT 'h'\n+          UNION ALL\n+          SELECT 'j'\n+      )\n+\n+test_fail_simple_fix_union_before_and_after:\n+  fail_str: |\n+      SELECT 'a' UNION SELECT 'b'\n+  fix_str: |\n+      SELECT 'a'\n+      UNION\n+      SELECT 'b'\n+\n+test_fail_simple_fix_intersect_before_and_after:\n+  fail_str: |\n+      SELECT 'a' INTERSECT SELECT 'b'\n+  fix_str: |\n+      SELECT 'a'\n+      INTERSECT\n+      SELECT 'b'\n+\n+test_fail_simple_fix_except_before_and_after:\n+  fail_str: |\n+      SELECT 'a' EXCEPT SELECT 'b'\n+  fix_str: |\n+      SELECT 'a'\n+      EXCEPT\n+      SELECT 'b'\n+\n+test_fail_simple_fix_minus_before_and_after:\n+  fail_str: |\n+      SELECT 'a' EXCEPT SELECT 'b'\n+  fix_str: |\n+      SELECT 'a'\n+      EXCEPT\n+      SELECT 'b'\n+\n+test_fail_simple_fix_bigquery_intersect_distinct_before_and_after:\n+  fail_str: |\n+      SELECT 'a' INTERSECT DISTINCT SELECT 'b'\n+  fix_str: |\n+      SELECT 'a'\n+      INTERSECT DISTINCT\n+      SELECT 'b'\n+  configs:\n+    core:\n+      dialect: bigquery\n+\n+test_fail_autofix_in_tsql_disabled:\n+  fail_str: |\n+    SELECT supplyID, supplier\n+    FROM dbo.SUPPLY1\n+    UNION ALL\n+    SELECT supplyID, supplier\n+    FROM dbo.SUPPLY2\n+  configs:\n+    core:\n+      dialect: tsql\ndiff --git a/test/rules/std_L003_L065_combo_test.py b/test/rules/std_L003_L065_combo_test.py\nnew file mode 100644\n--- /dev/null\n+++ b/test/rules/std_L003_L065_combo_test.py\n@@ -0,0 +1,46 @@\n+\"\"\"Tests the combination of L003 and L065.\n+\n+L003: Indentation not consistent with previous lines\n+L065: Set operators should be surrounded by newlines\n+\n+Auto fix of L065 does not insert correct indentation but just Newlines. It relies on\n+L003 to sort out the indentation later. This is what is getting tested here.\n+\"\"\"\n+\n+import sqlfluff\n+\n+\n+def test__rules__std_L003_L065_union_all_in_subquery_lint():\n+    \"\"\"Verify a that L065 reports lint errors in subqueries.\"\"\"\n+    sql = (\n+        \"SELECT * FROM (\\n\"\n+        \"    SELECT 'g' UNION ALL\\n\"\n+        \"    SELECT 'h'\\n\"\n+        \"    UNION ALL SELECT 'j'\\n\"\n+        \")\\n\"\n+    )\n+    result = sqlfluff.lint(sql)\n+\n+    assert \"L065\" in [r[\"code\"] for r in result]\n+\n+\n+def test__rules__std_L003_L065_union_all_in_subquery_fix():\n+    \"\"\"Verify combination of rules L003 and L065 produces a correct indentation.\"\"\"\n+    sql = (\n+        \"SELECT * FROM (\\n\"\n+        \"    SELECT 'g' UNION ALL\\n\"\n+        \"    SELECT 'h'\\n\"\n+        \"    UNION ALL SELECT 'j'\\n\"\n+        \")\\n\"\n+    )\n+    fixed_sql = (\n+        \"SELECT * FROM (\\n\"\n+        \"    SELECT 'g'\\n\"\n+        \"    UNION ALL\\n\"\n+        \"    SELECT 'h'\\n\"\n+        \"    UNION ALL\\n\"\n+        \"    SELECT 'j'\\n\"\n+        \")\\n\"\n+    )\n+    result = sqlfluff.fix(sql)\n+    assert result == fixed_sql\n",
        "problem_statement": "Rule suggestion: `UNION [ALL|DISTINCT]` on new line\n### Search before asking\n\n- [X] I searched the [issues](https://github.com/sqlfluff/sqlfluff/issues) and found no similar issues.\n\n\n### Description\n\nI would like to suggest a new rule that puts `UNION [ALL|DISTINCT]` statements on their own line, aligned to the surrounding `SELECT` statements.\r\n\r\nFor example, currently \r\n\r\n```sql\r\nSELECT 1 UNION ALL\r\nSELECT 2\r\n```\r\n\r\npasses without errors. This new rule could fix that to\r\n\r\n```sql\r\nSELECT 1 \r\nUNION ALL\r\nSELECT 2\r\n```\r\n\r\nOr in a more complex example\r\n\r\n```sql\r\nSELECT * FROM (\r\n    SELECT 1 UNION ALL\r\n    SELECT 2\r\n)\r\n```\r\n\r\nfixed to\r\n\r\n```sql\r\nSELECT * FROM (\r\n    SELECT 1 \r\n    UNION ALL\r\n    SELECT 2\r\n)\r\n```\n\n### Use case\n\nI have looked at a few SQL style guides and they don't really seem to mention any policy regarding `UNION` statements. However, in 99% of the SQL I have encountered `UNION` statements always seemed to be on a new line. It would be great to have an option to lint the remaining 1% \ud83d\ude09 \n\n### Dialect\n\nansi\n\n### Are you willing to work on and submit a PR to address the issue?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [X] I agree to follow this project's [Code of Conduct](https://github.com/sqlfluff/sqlfluff/blob/main/CODE_OF_CONDUCT.md)\n\n",
        "hints_text": "",
        "created_at": "2022-05-12T05:50:48Z",
        "version": "0.12",
        "FAIL_TO_PASS": [
            "test/rules/std_L003_L065_combo_test.py::test__rules__std_L003_L065_union_all_in_subquery_lint",
            "test/rules/std_L003_L065_combo_test.py::test__rules__std_L003_L065_union_all_in_subquery_fix"
        ],
        "PASS_TO_PASS": [],
        "environment_setup_commit": "8f6fd1d8a8d69b2c463fbcf5bd1131c47f12ad88"
    },
    {
        "repo": "pvlib/pvlib-python",
        "instance_id": "pvlib__pvlib-python-807",
        "base_commit": "e326fa53038f616d949e4f981dab6187d2ca9470",
        "patch": "diff --git a/pvlib/scaling.py b/pvlib/scaling.py\nnew file mode 100644\n--- /dev/null\n+++ b/pvlib/scaling.py\n@@ -0,0 +1,242 @@\n+\"\"\"\n+The ``scaling`` module contains functions for manipulating irradiance\n+or other variables to account for temporal or spatial characteristics.\n+\"\"\"\n+\n+import numpy as np\n+import pandas as pd\n+\n+\n+def wvm(clearsky_index, positions, cloud_speed, dt=None):\n+    \"\"\"\n+    Compute spatial aggregation time series smoothing on clear sky index based\n+    on the Wavelet Variability model of Lave et al [1-2]. Implementation is\n+    basically a port of the Matlab version of the code [3].\n+\n+    Parameters\n+    ----------\n+    clearsky_index : numeric or pandas.Series\n+        Clear Sky Index time series that will be smoothed.\n+\n+    positions : numeric\n+        Array of coordinate distances as (x,y) pairs representing the\n+        easting, northing of the site positions in meters [m]. Distributed\n+        plants could be simulated by gridded points throughout the plant\n+        footprint.\n+\n+    cloud_speed : numeric\n+        Speed of cloud movement in meters per second [m/s].\n+\n+    dt : float, default None\n+        The time series time delta. By default, is inferred from the\n+        clearsky_index. Must be specified for a time series that doesn't\n+        include an index. Units of seconds [s].\n+\n+    Returns\n+    -------\n+    smoothed : numeric or pandas.Series\n+        The Clear Sky Index time series smoothed for the described plant.\n+\n+    wavelet: numeric\n+        The individual wavelets for the time series before smoothing.\n+\n+    tmscales: numeric\n+        The timescales associated with the wavelets in seconds [s].\n+\n+    References\n+    ----------\n+    [1] M. Lave, J. Kleissl and J.S. Stein. A Wavelet-Based Variability\n+    Model (WVM) for Solar PV Power Plants. IEEE Transactions on Sustainable\n+    Energy, vol. 4, no. 2, pp. 501-509, 2013.\n+\n+    [2] M. Lave and J. Kleissl. Cloud speed impact on solar variability\n+    scaling - Application to the wavelet variability model. Solar Energy,\n+    vol. 91, pp. 11-21, 2013.\n+\n+    [3] Wavelet Variability Model - Matlab Code:\n+    https://pvpmc.sandia.gov/applications/wavelet-variability-model/\n+    \"\"\"\n+\n+    # Added by Joe Ranalli (@jranalli), Penn State Hazleton, 2019\n+\n+    try:\n+        import scipy.optimize\n+        from scipy.spatial.distance import pdist\n+    except ImportError:\n+        raise ImportError(\"The WVM function requires scipy.\")\n+\n+    pos = np.array(positions)\n+    dist = pdist(pos, 'euclidean')\n+    wavelet, tmscales = _compute_wavelet(clearsky_index, dt)\n+\n+    # Find effective length of position vector, 'dist' is full pairwise\n+    n_pairs = len(dist)\n+\n+    def fn(x):\n+        return np.abs((x ** 2 - x) / 2 - n_pairs)\n+    n_dist = np.round(scipy.optimize.fmin(fn, np.sqrt(n_pairs), disp=False))\n+\n+    # Compute VR\n+    A = cloud_speed / 2  # Resultant fit for A from [2]\n+    vr = np.zeros(tmscales.shape)\n+    for i, tmscale in enumerate(tmscales):\n+        rho = np.exp(-1 / A * dist / tmscale)  # Eq 5 from [1]\n+\n+        # 2*rho is because rho_ij = rho_ji. +n_dist accounts for sum(rho_ii=1)\n+        denominator = 2 * np.sum(rho) + n_dist\n+        vr[i] = n_dist ** 2 / denominator  # Eq 6 of [1]\n+\n+    # Scale each wavelet by VR (Eq 7 in [1])\n+    wavelet_smooth = np.zeros_like(wavelet)\n+    for i in np.arange(len(tmscales)):\n+        if i < len(tmscales) - 1:  # Treat the lowest freq differently\n+            wavelet_smooth[i, :] = wavelet[i, :] / np.sqrt(vr[i])\n+        else:\n+            wavelet_smooth[i, :] = wavelet[i, :]\n+\n+    outsignal = np.sum(wavelet_smooth, 0)\n+\n+    try:  # See if there's an index already, if so, return as a pandas Series\n+        smoothed = pd.Series(outsignal, index=clearsky_index.index)\n+    except AttributeError:\n+        smoothed = outsignal  # just output the numpy signal\n+\n+    return smoothed, wavelet, tmscales\n+\n+\n+def latlon_to_xy(coordinates):\n+    \"\"\"\n+    Convert latitude and longitude in degrees to a coordinate system measured\n+    in meters from zero deg latitude, zero deg longitude.\n+\n+    This is a convenience method to support inputs to wvm. Note that the\n+    methodology used is only suitable for short distances. For conversions of\n+    longer distances, users should consider use of Universal Transverse\n+    Mercator (UTM) or other suitable cartographic projection. Consider\n+    packages built for cartographic projection such as pyproj (e.g.\n+    pyproj.transform()) [2].\n+\n+    Parameters\n+    ----------\n+\n+    coordinates : numeric\n+        Array or list of (latitude, longitude) coordinate pairs. Use decimal\n+        degrees notation.\n+\n+    Returns\n+    -------\n+    xypos : numeric\n+        Array of coordinate distances as (x,y) pairs representing the\n+        easting, northing of the position in meters [m].\n+\n+    References\n+    ----------\n+    [1] H. Moritz. Geodetic Reference System 1980, Journal of Geodesy, vol. 74,\n+    no. 1, pp 128\u2013133, 2000.\n+\n+    [2] https://pypi.org/project/pyproj/\n+\n+    [3] Wavelet Variability Model - Matlab Code:\n+    https://pvpmc.sandia.gov/applications/wavelet-variability-model/\n+    \"\"\"\n+\n+    # Added by Joe Ranalli (@jranalli), Penn State Hazleton, 2019\n+\n+    r_earth = 6371008.7714  # mean radius of Earth, in meters\n+    m_per_deg_lat = r_earth * np.pi / 180\n+    try:\n+        meanlat = np.mean([lat for (lat, lon) in coordinates])  # Mean latitude\n+    except TypeError:  # Assume it's a single value?\n+        meanlat = coordinates[0]\n+    m_per_deg_lon = r_earth * np.cos(np.pi/180 * meanlat) * np.pi/180\n+\n+    # Conversion\n+    pos = coordinates * np.array(m_per_deg_lat, m_per_deg_lon)\n+\n+    # reshape as (x,y) pairs to return\n+    try:\n+        return np.column_stack([pos[:, 1], pos[:, 0]])\n+    except IndexError:  # Assume it's a single value, which has a 1D shape\n+        return np.array((pos[1], pos[0]))\n+\n+\n+def _compute_wavelet(clearsky_index, dt=None):\n+    \"\"\"\n+    Compute the wavelet transform on the input clear_sky time series.\n+\n+    Parameters\n+    ----------\n+    clearsky_index : numeric or pandas.Series\n+        Clear Sky Index time series that will be smoothed.\n+\n+    dt : float, default None\n+        The time series time delta. By default, is inferred from the\n+        clearsky_index. Must be specified for a time series that doesn't\n+        include an index. Units of seconds [s].\n+\n+    Returns\n+    -------\n+    wavelet: numeric\n+        The individual wavelets for the time series\n+\n+    tmscales: numeric\n+        The timescales associated with the wavelets in seconds [s]\n+\n+    References\n+    ----------\n+    [1] M. Lave, J. Kleissl and J.S. Stein. A Wavelet-Based Variability\n+    Model (WVM) for Solar PV Power Plants. IEEE Transactions on Sustainable\n+    Energy, vol. 4, no. 2, pp. 501-509, 2013.\n+\n+    [3] Wavelet Variability Model - Matlab Code:\n+    https://pvpmc.sandia.gov/applications/wavelet-variability-model/\n+    \"\"\"\n+\n+    # Added by Joe Ranalli (@jranalli), Penn State Hazleton, 2019\n+\n+    try:  # Assume it's a pandas type\n+        vals = clearsky_index.values.flatten()\n+    except AttributeError:  # Assume it's a numpy type\n+        vals = clearsky_index.flatten()\n+        if dt is None:\n+            raise ValueError(\"dt must be specified for numpy type inputs.\")\n+    else:  # flatten() succeeded, thus it's a pandas type, so get its dt\n+        try:  # Assume it's a time series type index\n+            dt = (clearsky_index.index[1] - clearsky_index.index[0]).seconds\n+        except AttributeError:  # It must just be a numeric index\n+            dt = (clearsky_index.index[1] - clearsky_index.index[0])\n+\n+    # Pad the series on both ends in time and place in a dataframe\n+    cs_long = np.pad(vals, (len(vals), len(vals)), 'symmetric')\n+    cs_long = pd.DataFrame(cs_long)\n+\n+    # Compute wavelet time scales\n+    min_tmscale = np.ceil(np.log(dt)/np.log(2))  # Minimum wavelet timescale\n+    max_tmscale = int(12 - min_tmscale)  # maximum wavelet timescale\n+\n+    tmscales = np.zeros(max_tmscale)\n+    csi_mean = np.zeros([max_tmscale, len(cs_long)])\n+    # Loop for all time scales we will consider\n+    for i in np.arange(0, max_tmscale):\n+        j = i+1\n+        tmscales[i] = 2**j * dt  # Wavelet integration time scale\n+        intvlen = 2**j  # Wavelet integration time series interval\n+        # Rolling average, retains only lower frequencies than interval\n+        df = cs_long.rolling(window=intvlen, center=True, min_periods=1).mean()\n+        # Fill nan's in both directions\n+        df = df.fillna(method='bfill').fillna(method='ffill')\n+        # Pop values back out of the dataframe and store\n+        csi_mean[i, :] = df.values.flatten()\n+\n+    # Calculate the wavelets by isolating the rolling mean frequency ranges\n+    wavelet_long = np.zeros(csi_mean.shape)\n+    for i in np.arange(0, max_tmscale-1):\n+        wavelet_long[i, :] = csi_mean[i, :] - csi_mean[i+1, :]\n+    wavelet_long[max_tmscale-1, :] = csi_mean[max_tmscale-1, :]  # Lowest freq\n+\n+    # Clip off the padding and just return the original time window\n+    wavelet = np.zeros([max_tmscale, len(vals)])\n+    for i in np.arange(0, max_tmscale):\n+        wavelet[i, :] = wavelet_long[i, len(vals)+1: 2*len(vals)+1]\n+\n+    return wavelet, tmscales\n",
        "test_patch": "diff --git a/pvlib/test/test_scaling.py b/pvlib/test/test_scaling.py\nnew file mode 100644\n--- /dev/null\n+++ b/pvlib/test/test_scaling.py\n@@ -0,0 +1,146 @@\n+import numpy as np\n+import pandas as pd\n+\n+import pytest\n+from numpy.testing import assert_almost_equal\n+\n+from pvlib import scaling\n+from conftest import requires_scipy\n+\n+\n+# Sample cloud speed\n+cloud_speed = 5\n+\n+# Sample dt\n+dt = 1\n+\n+\n+@pytest.fixture\n+def coordinates():\n+    # Sample positions in lat/lon\n+    lat = np.array((9.99, 10, 10.01))\n+    lon = np.array((4.99, 5, 5.01))\n+    coordinates = np.array([(lati, loni) for (lati, loni) in zip(lat, lon)])\n+    return coordinates\n+\n+\n+@pytest.fixture\n+def clear_sky_index():\n+    # Generate a sample clear_sky_index\n+    clear_sky_index = np.ones(10000)\n+    clear_sky_index[5000:5005] = np.array([1, 1, 1.1, 0.9, 1])\n+    return clear_sky_index\n+\n+\n+@pytest.fixture\n+def time(clear_sky_index):\n+    # Sample time vector\n+    return np.arange(0, len(clear_sky_index))\n+\n+\n+@pytest.fixture\n+def positions():\n+    # Sample positions based on the previous lat/lon (calculated manually)\n+    expect_xpos = np.array([554863.4, 555975.4, 557087.3])\n+    expect_ypos = np.array([1110838.8, 1111950.8, 1113062.7])\n+    return np.array([pt for pt in zip(expect_xpos, expect_ypos)])\n+\n+\n+@pytest.fixture\n+def expect_tmscale():\n+    # Expected timescales for dt = 1\n+    return [2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048, 4096]\n+\n+\n+@pytest.fixture\n+def expect_wavelet():\n+    # Expected wavelet for indices 5000:5004 for clear_sky_index above (Matlab)\n+    return np.array([[-0.025, 0.05, 0., -0.05, 0.025],\n+                     [0.025, 0., 0., 0., -0.025],\n+                     [0., 0., 0., 0., 0.]])\n+\n+\n+@pytest.fixture\n+def expect_cs_smooth():\n+    # Expected smoothed clear sky index for indices 5000:5004 (Matlab)\n+    return np.array([1., 1.0289, 1., 0.9711, 1.])\n+\n+\n+def test_latlon_to_xy_zero():\n+    coord = [0, 0]\n+    pos_e = [0, 0]\n+    pos = scaling.latlon_to_xy(coord)\n+    assert_almost_equal(pos, pos_e, decimal=1)\n+\n+\n+def test_latlon_to_xy_single(coordinates, positions):\n+    # Must test against central value, because latlon_to_xy uses the mean\n+    coord = coordinates[1]\n+    pos = scaling.latlon_to_xy(coord)\n+    assert_almost_equal(pos, positions[1], decimal=1)\n+\n+\n+def test_latlon_to_xy_array(coordinates, positions):\n+    pos = scaling.latlon_to_xy(coordinates)\n+    assert_almost_equal(pos, positions, decimal=1)\n+\n+\n+def test_latlon_to_xy_list(coordinates, positions):\n+    pos = scaling.latlon_to_xy(coordinates.tolist())\n+    assert_almost_equal(pos, positions, decimal=1)\n+\n+\n+def test_compute_wavelet_series(clear_sky_index, time,\n+                                expect_tmscale, expect_wavelet):\n+    csi_series = pd.Series(clear_sky_index, index=time)\n+    wavelet, tmscale = scaling._compute_wavelet(csi_series)\n+    assert_almost_equal(tmscale, expect_tmscale)\n+    assert_almost_equal(wavelet[0:3, 5000:5005], expect_wavelet)\n+\n+\n+def test_compute_wavelet_series_numindex(clear_sky_index, time,\n+                                         expect_tmscale, expect_wavelet):\n+    dtindex = pd.to_datetime(time, unit='s')\n+    csi_series = pd.Series(clear_sky_index, index=dtindex)\n+    wavelet, tmscale = scaling._compute_wavelet(csi_series)\n+    assert_almost_equal(tmscale, expect_tmscale)\n+    assert_almost_equal(wavelet[0:3, 5000:5005], expect_wavelet)\n+\n+\n+def test_compute_wavelet_array(clear_sky_index,\n+                               expect_tmscale, expect_wavelet):\n+    wavelet, tmscale = scaling._compute_wavelet(clear_sky_index, dt)\n+    assert_almost_equal(tmscale, expect_tmscale)\n+    assert_almost_equal(wavelet[0:3, 5000:5005], expect_wavelet)\n+\n+\n+def test_compute_wavelet_array_invalid(clear_sky_index):\n+    with pytest.raises(ValueError):\n+        scaling._compute_wavelet(clear_sky_index)\n+\n+\n+@requires_scipy\n+def test_wvm_series(clear_sky_index, time, positions, expect_cs_smooth):\n+    csi_series = pd.Series(clear_sky_index, index=time)\n+    cs_sm, _, _ = scaling.wvm(csi_series, positions, cloud_speed)\n+    assert_almost_equal(cs_sm[5000:5005], expect_cs_smooth, decimal=4)\n+\n+\n+@requires_scipy\n+def test_wvm_array(clear_sky_index, positions, expect_cs_smooth):\n+    cs_sm, _, _ = scaling.wvm(clear_sky_index, positions, cloud_speed, dt=dt)\n+    assert_almost_equal(cs_sm[5000:5005], expect_cs_smooth, decimal=4)\n+\n+\n+@requires_scipy\n+def test_wvm_series_xyaslist(clear_sky_index, time, positions,\n+                             expect_cs_smooth):\n+    csi_series = pd.Series(clear_sky_index, index=time)\n+    cs_sm, _, _ = scaling.wvm(csi_series, positions.tolist(), cloud_speed)\n+    assert_almost_equal(cs_sm[5000:5005], expect_cs_smooth, decimal=4)\n+\n+\n+@requires_scipy\n+def test_wvm_invalid(clear_sky_index, positions):\n+    with pytest.raises(ValueError):\n+        scaling.wvm(clear_sky_index, positions, cloud_speed)\n",
        "problem_statement": "Add Wavelet Variability Model (WVM) for calculating spatial smoothing of irradiance\n> > Should I spin this off to a separate issue, since it might be different (and more compartmented) than the broader downscaling discussion?\r\n> \r\n> Yes. Let's start a new module with this submission, `scaling.py` comes to mind, but I'm not enamored of it. Scope will be functions that operate on irradiance, perhaps other variables, to transform temporal or spatial characteristics.\r\n\r\nSpinoff from [issue #788 ](https://github.com/pvlib/pvlib-python/issues/788). Implementation is a python port of WVM, released as an auxiliary to the Matlab pvlib [here](https://pvpmc.sandia.gov/applications/wavelet-variability-model/). My implementation ports the original model logic, but deviates from the overall package, in that I begin at the point where the user already has a clear sky index to operate on (original starts from GHI and calculates POA clear sky index). I thought this would allow for more flexibility in choice of transposition model, etc, but it does ask a bit more work up front for a user to run the WVM.\r\n\r\nI am close to completion of a draft and will create a pull request when ready. This is my first contribution to the project (or any open source project really), so please accept my apologies in advance if it takes some guidance.\n",
        "hints_text": "> This is my first contribution to the project (or any open source project really), so please accept my apologies in advance if it takes some guidance.\r\n\r\nWelcome!  Asking for a clear-sky index as input seems appropriate; there's no need to rigidly follow the MATLAB implementation. I'll ask for your patience with the review process, which can involve multiple iterations and reviewers.",
        "created_at": "2019-11-01T14:54:52Z",
        "version": "0.6",
        "FAIL_TO_PASS": [
            "pvlib/test/test_scaling.py::test_latlon_to_xy_zero",
            "pvlib/test/test_scaling.py::test_latlon_to_xy_single",
            "pvlib/test/test_scaling.py::test_latlon_to_xy_array",
            "pvlib/test/test_scaling.py::test_latlon_to_xy_list",
            "pvlib/test/test_scaling.py::test_compute_wavelet_series",
            "pvlib/test/test_scaling.py::test_compute_wavelet_series_numindex",
            "pvlib/test/test_scaling.py::test_compute_wavelet_array",
            "pvlib/test/test_scaling.py::test_compute_wavelet_array_invalid",
            "pvlib/test/test_scaling.py::test_wvm_series",
            "pvlib/test/test_scaling.py::test_wvm_array",
            "pvlib/test/test_scaling.py::test_wvm_series_xyaslist",
            "pvlib/test/test_scaling.py::test_wvm_invalid"
        ],
        "PASS_TO_PASS": [],
        "environment_setup_commit": "b91d178868d193afd56f8e3b013661a473d699c3"
    },
    {
        "repo": "pvlib/pvlib-python",
        "instance_id": "pvlib__pvlib-python-718",
        "base_commit": "3c84edd644fa4db54955e2225a183fa3e0405eb0",
        "patch": "diff --git a/pvlib/__init__.py b/pvlib/__init__.py\n--- a/pvlib/__init__.py\n+++ b/pvlib/__init__.py\n@@ -7,6 +7,7 @@\n from pvlib import location\n from pvlib import solarposition\n from pvlib import iotools\n+from pvlib import ivtools\n from pvlib import tracking\n from pvlib import pvsystem\n from pvlib import spa\ndiff --git a/pvlib/ivtools.py b/pvlib/ivtools.py\nnew file mode 100644\n--- /dev/null\n+++ b/pvlib/ivtools.py\n@@ -0,0 +1,350 @@\n+# -*- coding: utf-8 -*-\n+\"\"\"\n+Created on Fri Mar 29 10:34:10 2019\n+\n+@author: cwhanse\n+\"\"\"\n+\n+import numpy as np\n+\n+\n+def fit_sdm_cec_sam(celltype, v_mp, i_mp, v_oc, i_sc, alpha_sc, beta_voc,\n+                    gamma_pmp, cells_in_series, temp_ref=25):\n+    \"\"\"\n+    Estimates parameters for the CEC single diode model (SDM) using the SAM\n+    SDK.\n+\n+    Parameters\n+    ----------\n+    celltype : str\n+        Value is one of 'monoSi', 'multiSi', 'polySi', 'cis', 'cigs', 'cdte',\n+        'amorphous'\n+    v_mp : float\n+        Voltage at maximum power point [V]\n+    i_mp : float\n+        Current at maximum power point [A]\n+    v_oc : float\n+        Open circuit voltage [V]\n+    i_sc : float\n+        Short circuit current [A]\n+    alpha_sc : float\n+        Temperature coefficient of short circuit current [A/C]\n+    beta_voc : float\n+        Temperature coefficient of open circuit voltage [V/C]\n+    gamma_pmp : float\n+        Temperature coefficient of power at maximum point point [%/C]\n+    cells_in_series : int\n+        Number of cells in series\n+    temp_ref : float, default 25\n+        Reference temperature condition [C]\n+\n+    Returns\n+    -------\n+    tuple of the following elements:\n+\n+        * I_L_ref : float\n+            The light-generated current (or photocurrent) at reference\n+            conditions [A]\n+\n+        * I_o_ref : float\n+            The dark or diode reverse saturation current at reference\n+            conditions [A]\n+\n+        * R_sh_ref : float\n+            The shunt resistance at reference conditions, in ohms.\n+\n+        * R_s : float\n+            The series resistance at reference conditions, in ohms.\n+\n+        * a_ref : float\n+            The product of the usual diode ideality factor ``n`` (unitless),\n+            number of cells in series ``Ns``, and cell thermal voltage at\n+            reference conditions [V]\n+\n+        * Adjust : float\n+            The adjustment to the temperature coefficient for short circuit\n+            current, in percent.\n+\n+    Raises\n+    ------\n+        ImportError if NREL-PySAM is not installed.\n+\n+        RuntimeError if parameter extraction is not successful.\n+\n+    Notes\n+    -----\n+    Inputs ``v_mp``, ``v_oc``, ``i_mp`` and ``i_sc`` are assumed to be from a\n+    single IV curve at constant irradiance and cell temperature. Irradiance is\n+    not explicitly used by the fitting procedure. The irradiance level at which\n+    the input IV curve is determined and the specified cell temperature\n+    ``temp_ref`` are the reference conditions for the output parameters\n+    ``I_L_ref``, ``I_o_ref``, ``R_sh_ref``, ``R_s``, ``a_ref`` and ``Adjust``.\n+\n+    References\n+    ----------\n+    [1] A. Dobos, \"An Improved Coefficient Calculator for the California\n+    Energy Commission 6 Parameter Photovoltaic Module Model\", Journal of\n+    Solar Energy Engineering, vol 134, 2012.\n+    \"\"\"\n+\n+    try:\n+        from PySAM import PySSC\n+    except ImportError:\n+        raise ImportError(\"Requires NREL's PySAM package at \"\n+                          \"https://pypi.org/project/NREL-PySAM/.\")\n+\n+    datadict = {'tech_model': '6parsolve', 'financial_model': 'none',\n+                'celltype': celltype, 'Vmp': v_mp,\n+                'Imp': i_mp, 'Voc': v_oc, 'Isc': i_sc, 'alpha_isc': alpha_sc,\n+                'beta_voc': beta_voc, 'gamma_pmp': gamma_pmp,\n+                'Nser': cells_in_series, 'Tref': temp_ref}\n+\n+    result = PySSC.ssc_sim_from_dict(datadict)\n+    if result['cmod_success'] == 1:\n+        return tuple([result[k] for k in ['Il', 'Io', 'Rsh', 'Rs', 'a',\n+                      'Adj']])\n+    else:\n+        raise RuntimeError('Parameter estimation failed')\n+\n+\n+def fit_sde_sandia(voltage, current, v_oc=None, i_sc=None, v_mp_i_mp=None,\n+                   vlim=0.2, ilim=0.1):\n+    r\"\"\"\n+    Fits the single diode equation (SDE) to an IV curve.\n+\n+    Parameters\n+    ----------\n+    voltage : ndarray\n+        1D array of `float` type containing voltage at each point on the IV\n+        curve, increasing from 0 to ``v_oc`` inclusive [V]\n+\n+    current : ndarray\n+        1D array of `float` type containing current at each point on the IV\n+        curve, from ``i_sc`` to 0 inclusive [A]\n+\n+    v_oc : float, default None\n+        Open circuit voltage [V]. If not provided, ``v_oc`` is taken as the\n+        last point in the ``voltage`` array.\n+\n+    i_sc : float, default None\n+        Short circuit current [A]. If not provided, ``i_sc`` is taken as the\n+        first point in the ``current`` array.\n+\n+    v_mp_i_mp : tuple of float, default None\n+        Voltage, current at maximum power point in units of [V], [A].\n+        If not provided, the maximum power point is found at the maximum of\n+        ``voltage`` \\times ``current``.\n+\n+    vlim : float, default 0.2\n+        Defines portion of IV curve where the exponential term in the single\n+        diode equation can be neglected, i.e.\n+        ``voltage`` <= ``vlim`` x ``v_oc`` [V]\n+\n+    ilim : float, default 0.1\n+        Defines portion of the IV curve where the exponential term in the\n+        single diode equation is signficant, approximately defined by\n+        ``current`` < (1 - ``ilim``) x ``i_sc`` [A]\n+\n+    Returns\n+    -------\n+    tuple of the following elements:\n+\n+        * photocurrent : float\n+            photocurrent [A]\n+        * saturation_current : float\n+            dark (saturation) current [A]\n+        * resistance_shunt : float\n+            shunt (parallel) resistance, in ohms\n+        * resistance_series : float\n+            series resistance, in ohms\n+        * nNsVth : float\n+            product of thermal voltage ``Vth`` [V], diode ideality factor\n+            ``n``, and number of series cells ``Ns``\n+\n+    Raises\n+    ------\n+    RuntimeError if parameter extraction is not successful.\n+\n+    Notes\n+    -----\n+    Inputs ``voltage``, ``current``, ``v_oc``, ``i_sc`` and ``v_mp_i_mp`` are\n+    assumed to be from a single IV curve at constant irradiance and cell\n+    temperature.\n+\n+    :py:func:`fit_single_diode_sandia` obtains values for the five parameters\n+    for the single diode equation [1]:\n+\n+    .. math::\n+\n+        I = I_{L} - I_{0} (\\exp \\frac{V + I R_{s}}{nNsVth} - 1)\n+        - \\frac{V + I R_{s}}{R_{sh}}\n+\n+    See :py:func:`pvsystem.singlediode` for definition of the parameters.\n+\n+    The extraction method [2] proceeds in six steps.\n+\n+    1. In the single diode equation, replace :math:`R_{sh} = 1/G_{p}` and\n+       re-arrange\n+\n+    .. math::\n+\n+        I = \\frac{I_{L}}{1 + G_{p} R_{s}} - \\frac{G_{p} V}{1 + G_{p} R_{s}}\n+        - \\frac{I_{0}}{1 + G_{p} R_{s}} (\\exp(\\frac{V + I R_{s}}{nNsVth}) - 1)\n+\n+    2. The linear portion of the IV curve is defined as\n+       :math:`V \\le vlim \\times v_oc`. Over this portion of the IV curve,\n+\n+    .. math::\n+\n+        \\frac{I_{0}}{1 + G_{p} R_{s}} (\\exp(\\frac{V + I R_{s}}{nNsVth}) - 1)\n+        \\approx 0\n+\n+    3. Fit the linear portion of the IV curve with a line.\n+\n+    .. math::\n+\n+        I &\\approx \\frac{I_{L}}{1 + G_{p} R_{s}} - \\frac{G_{p} V}{1 + G_{p}\n+        R_{s}} \\\\\n+        &= \\beta_{0} + \\beta_{1} V\n+\n+    4. The exponential portion of the IV curve is defined by\n+       :math:`\\beta_{0} + \\beta_{1} \\times V - I > ilim \\times i_sc`.\n+       Over this portion of the curve, :math:`exp((V + IRs)/nNsVth) >> 1`\n+       so that\n+\n+    .. math::\n+\n+        \\exp(\\frac{V + I R_{s}}{nNsVth}) - 1 \\approx\n+        \\exp(\\frac{V + I R_{s}}{nNsVth})\n+\n+    5. Fit the exponential portion of the IV curve.\n+\n+    .. math::\n+\n+        \\log(\\beta_{0} - \\beta_{1} V - I)\n+        &\\approx \\log(\\frac{I_{0}}{1 + G_{p} R_{s}} + \\frac{V}{nNsVth}\n+        + \\frac{I R_{s}}{nNsVth} \\\\\n+        &= \\beta_{2} + beta_{3} V + \\beta_{4} I\n+\n+    6. Calculate values for ``IL, I0, Rs, Rsh,`` and ``nNsVth`` from the\n+       regression coefficents :math:`\\beta_{0}, \\beta_{1}, \\beta_{3}` and\n+       :math:`\\beta_{4}`.\n+\n+\n+    References\n+    ----------\n+    [1] S.R. Wenham, M.A. Green, M.E. Watt, \"Applied Photovoltaics\" ISBN\n+    0 86758 909 4\n+    [2] C. B. Jones, C. W. Hansen, Single Diode Parameter Extraction from\n+    In-Field Photovoltaic I-V Curves on a Single Board Computer, 46th IEEE\n+    Photovoltaic Specialist Conference, Chicago, IL, 2019\n+    \"\"\"\n+\n+    # If not provided, extract v_oc, i_sc, v_mp and i_mp from the IV curve data\n+    if v_oc is None:\n+        v_oc = voltage[-1]\n+    if i_sc is None:\n+        i_sc = current[0]\n+    if v_mp_i_mp is not None:\n+        v_mp, i_mp = v_mp_i_mp\n+    else:\n+        v_mp, i_mp = _find_mp(voltage, current)\n+\n+    # Find beta0 and beta1 from linear portion of the IV curve\n+    beta0, beta1 = _find_beta0_beta1(voltage, current, vlim, v_oc)\n+\n+    # Find beta3 and beta4 from the exponential portion of the IV curve\n+    beta3, beta4 = _find_beta3_beta4(voltage, current, beta0, beta1, ilim,\n+                                     i_sc)\n+\n+    # calculate single diode parameters from regression coefficients\n+    return _calculate_sde_parameters(beta0, beta1, beta3, beta4, v_mp, i_mp,\n+                                     v_oc)\n+\n+\n+def _find_mp(voltage, current):\n+    \"\"\"\n+    Finds voltage and current at maximum power point.\n+\n+    Parameters\n+    ----------\n+    voltage : ndarray\n+        1D array containing voltage at each point on the IV curve, increasing\n+        from 0 to v_oc inclusive, of `float` type [V]\n+\n+    current : ndarray\n+        1D array containing current at each point on the IV curve, decreasing\n+        from i_sc to 0 inclusive, of `float` type [A]\n+\n+    Returns\n+    -------\n+    v_mp, i_mp : tuple\n+        voltage ``v_mp`` and current ``i_mp`` at the maximum power point [V],\n+        [A]\n+    \"\"\"\n+    p = voltage * current\n+    idx = np.argmax(p)\n+    return voltage[idx], current[idx]\n+\n+\n+def _calc_I0(IL, I, V, Gp, Rs, nNsVth):\n+    return (IL - I - Gp * V - Gp * Rs * I) / np.exp((V + Rs * I) / nNsVth)\n+\n+\n+def _find_beta0_beta1(v, i, vlim, v_oc):\n+    # Get intercept and slope of linear portion of IV curve.\n+    # Start with V =< vlim * v_oc, extend by adding points until slope is\n+    # negative (downward).\n+    beta0 = np.nan\n+    beta1 = np.nan\n+    first_idx = np.searchsorted(v, vlim * v_oc)\n+    for idx in range(first_idx, len(v)):\n+        coef = np.polyfit(v[:idx], i[:idx], deg=1)\n+        if coef[0] < 0:\n+            # intercept term\n+            beta0 = coef[1].item()\n+            # sign change of slope to get positive parameter value\n+            beta1 = -coef[0].item()\n+            break\n+    if any(np.isnan([beta0, beta1])):\n+        raise RuntimeError(\"Parameter extraction failed: beta0={}, beta1={}\"\n+                           .format(beta0, beta1))\n+    else:\n+        return beta0, beta1\n+\n+\n+def _find_beta3_beta4(voltage, current, beta0, beta1, ilim, i_sc):\n+    # Subtract the IV curve from the linear fit.\n+    y = beta0 - beta1 * voltage - current\n+    x = np.array([np.ones_like(voltage), voltage, current]).T\n+    # Select points where y > ilim * i_sc to regress log(y) onto x\n+    idx = (y > ilim * i_sc)\n+    result = np.linalg.lstsq(x[idx], np.log(y[idx]), rcond=None)\n+    coef = result[0]\n+    beta3 = coef[1].item()\n+    beta4 = coef[2].item()\n+    if any(np.isnan([beta3, beta4])):\n+        raise RuntimeError(\"Parameter extraction failed: beta3={}, beta4={}\"\n+                           .format(beta3, beta4))\n+    else:\n+        return beta3, beta4\n+\n+\n+def _calculate_sde_parameters(beta0, beta1, beta3, beta4, v_mp, i_mp, v_oc):\n+    nNsVth = 1.0 / beta3\n+    Rs = beta4 / beta3\n+    Gp = beta1 / (1.0 - Rs * beta1)\n+    Rsh = 1.0 / Gp\n+    IL = (1 + Gp * Rs) * beta0\n+    # calculate I0\n+    I0_vmp = _calc_I0(IL, i_mp, v_mp, Gp, Rs, nNsVth)\n+    I0_voc = _calc_I0(IL, 0, v_oc, Gp, Rs, nNsVth)\n+    if any(np.isnan([I0_vmp, I0_voc])) or ((I0_vmp <= 0) and (I0_voc <= 0)):\n+        raise RuntimeError(\"Parameter extraction failed: I0 is undetermined.\")\n+    elif (I0_vmp > 0) and (I0_voc > 0):\n+        I0 = 0.5 * (I0_vmp + I0_voc)\n+    elif (I0_vmp > 0):\n+        I0 = I0_vmp\n+    else:  # I0_voc > 0\n+        I0 = I0_voc\n+    return (IL, I0, Rsh, Rs, nNsVth)\ndiff --git a/setup.py b/setup.py\n--- a/setup.py\n+++ b/setup.py\n@@ -44,8 +44,8 @@\n TESTS_REQUIRE = ['nose', 'pytest', 'pytest-cov', 'pytest-mock',\n                  'pytest-timeout']\n EXTRAS_REQUIRE = {\n-    'optional': ['ephem', 'cython', 'netcdf4', 'numba', 'pvfactors', 'scipy',\n-                 'siphon', 'tables'],\n+    'optional': ['ephem', 'cython', 'netcdf4', 'nrel-pysam', 'numba',\n+                 'pvfactors', 'scipy', 'siphon', 'tables'],\n     'doc': ['ipython', 'matplotlib', 'sphinx', 'sphinx_rtd_theme'],\n     'test': TESTS_REQUIRE\n }\n",
        "test_patch": "diff --git a/pvlib/test/conftest.py b/pvlib/test/conftest.py\n--- a/pvlib/test/conftest.py\n+++ b/pvlib/test/conftest.py\n@@ -151,6 +151,15 @@ def has_numba():\n                                         reason='requires pvfactors')\n \n \n+try:\n+    import PySAM  # noqa: F401\n+    has_pysam = True\n+except ImportError:\n+    has_pysam = False\n+\n+requires_pysam = pytest.mark.skipif(not has_pysam, reason=\"requires PySAM\")\n+\n+\n @pytest.fixture(scope=\"session\")\n def sam_data():\n     data = {}\ndiff --git a/pvlib/test/test_ivtools.py b/pvlib/test/test_ivtools.py\nnew file mode 100644\n--- /dev/null\n+++ b/pvlib/test/test_ivtools.py\n@@ -0,0 +1,242 @@\n+# -*- coding: utf-8 -*-\n+\"\"\"\n+Created on Thu May  9 10:51:15 2019\n+\n+@author: cwhanse\n+\"\"\"\n+\n+import numpy as np\n+import pandas as pd\n+import pytest\n+from pvlib import pvsystem\n+from pvlib import ivtools\n+from pvlib.test.conftest import requires_scipy, requires_pysam\n+\n+\n+@pytest.fixture\n+def get_test_iv_params():\n+    return {'IL': 8.0, 'I0': 5e-10, 'Rsh': 1000, 'Rs': 0.2, 'nNsVth': 1.61864}\n+\n+\n+@pytest.fixture\n+def get_cec_params_cansol_cs5p_220p():\n+    return {'input': {'V_mp_ref': 46.6, 'I_mp_ref': 4.73, 'V_oc_ref': 58.3,\n+                      'I_sc_ref': 5.05, 'alpha_sc': 0.0025,\n+                      'beta_voc': -0.19659, 'gamma_pmp': -0.43,\n+                      'cells_in_series': 96},\n+            'output': {'a_ref': 2.3674, 'I_L_ref': 5.056, 'I_o_ref': 1.01e-10,\n+                       'R_sh_ref': 837.51, 'R_s': 1.004, 'Adjust': 2.3}}\n+\n+\n+@requires_scipy\n+def test_fit_sde_sandia(get_test_iv_params, get_bad_iv_curves):\n+    test_params = get_test_iv_params\n+    testcurve = pvsystem.singlediode(photocurrent=test_params['IL'],\n+                                     saturation_current=test_params['I0'],\n+                                     resistance_shunt=test_params['Rsh'],\n+                                     resistance_series=test_params['Rs'],\n+                                     nNsVth=test_params['nNsVth'],\n+                                     ivcurve_pnts=300)\n+    expected = tuple(test_params[k] for k in ['IL', 'I0', 'Rsh', 'Rs',\n+                     'nNsVth'])\n+    result = ivtools.fit_sde_sandia(voltage=testcurve['v'],\n+                                    current=testcurve['i'])\n+    assert np.allclose(result, expected, rtol=5e-5)\n+    result = ivtools.fit_sde_sandia(voltage=testcurve['v'],\n+                                    current=testcurve['i'],\n+                                    v_oc=testcurve['v_oc'],\n+                                    i_sc=testcurve['i_sc'])\n+    assert np.allclose(result, expected, rtol=5e-5)\n+    result = ivtools.fit_sde_sandia(voltage=testcurve['v'],\n+                                    current=testcurve['i'],\n+                                    v_oc=testcurve['v_oc'],\n+                                    i_sc=testcurve['i_sc'],\n+                                    v_mp_i_mp=(testcurve['v_mp'],\n+                                    testcurve['i_mp']))\n+    assert np.allclose(result, expected, rtol=5e-5)\n+    result = ivtools.fit_sde_sandia(voltage=testcurve['v'],\n+                                    current=testcurve['i'], vlim=0.1)\n+    assert np.allclose(result, expected, rtol=5e-5)\n+\n+\n+@requires_scipy\n+def test_fit_sde_sandia_bad_iv(get_bad_iv_curves):\n+    # bad IV curves for coverage of if/then in _calculate_sde_parameters\n+    v1, i1, v2, i2 = get_bad_iv_curves\n+    result = ivtools.fit_sde_sandia(voltage=v1, current=i1)\n+    assert np.allclose(result, (-2.4322856072799985, 8.854688976836396,\n+                                -63.56227601452038, 111.18558915546389,\n+                                -137.9965046659527))\n+    result = ivtools.fit_sde_sandia(voltage=v2, current=i2)\n+    assert np.allclose(result, (2.62405311949227, 1.8657963912925288,\n+                                110.35202827739991, -65.652554411442,\n+                                174.49362093001415))\n+\n+\n+@requires_pysam\n+def test_fit_sdm_cec_sam(get_cec_params_cansol_cs5p_220p):\n+    input_data = get_cec_params_cansol_cs5p_220p['input']\n+    I_L_ref, I_o_ref, R_sh_ref, R_s, a_ref, Adjust = \\\n+        ivtools.fit_sdm_cec_sam(\n+            celltype='polySi', v_mp=input_data['V_mp_ref'],\n+            i_mp=input_data['I_mp_ref'], v_oc=input_data['V_oc_ref'],\n+            i_sc=input_data['I_sc_ref'], alpha_sc=input_data['alpha_sc'],\n+            beta_voc=input_data['beta_voc'],\n+            gamma_pmp=input_data['gamma_pmp'],\n+            cells_in_series=input_data['cells_in_series'])\n+    expected = pd.Series(get_cec_params_cansol_cs5p_220p['output'])\n+    modeled = pd.Series(index=expected.index, data=np.nan)\n+    modeled['a_ref'] = a_ref\n+    modeled['I_L_ref'] = I_L_ref\n+    modeled['I_o_ref'] = I_o_ref\n+    modeled['R_sh_ref'] = R_sh_ref\n+    modeled['R_s'] = R_s\n+    modeled['Adjust'] = Adjust\n+    assert np.allclose(modeled.values, expected.values, rtol=5e-2)\n+    # test for fitting failure\n+    with pytest.raises(RuntimeError):\n+        I_L_ref, I_o_ref, R_sh_ref, R_s, a_ref, Adjust = \\\n+            ivtools.fit_sdm_cec_sam(\n+                celltype='polySi', v_mp=0.45, i_mp=5.25, v_oc=0.55, i_sc=5.5,\n+                alpha_sc=0.00275, beta_voc=0.00275, gamma_pmp=0.0055,\n+                cells_in_series=1, temp_ref=25)\n+\n+\n+@pytest.fixture\n+def get_bad_iv_curves():\n+    # v1, i1 produces a bad value for I0_voc\n+    v1 = np.array([0, 0.338798867469060, 0.677597734938121, 1.01639660240718,\n+                   1.35519546987624, 1.69399433734530, 2.03279320481436,\n+                   2.37159207228342, 2.71039093975248, 3.04918980722154,\n+                   3.38798867469060, 3.72678754215966, 4.06558640962873,\n+                   4.40438527709779, 4.74318414456685, 5.08198301203591,\n+                   5.42078187950497, 5.75958074697403, 6.09837961444309,\n+                   6.43717848191215, 6.77597734938121, 7.11477621685027,\n+                   7.45357508431933, 7.79237395178839, 8.13117281925745,\n+                   8.46997168672651, 8.80877055419557, 9.14756942166463,\n+                   9.48636828913369, 9.82516715660275, 10.1639660240718,\n+                   10.5027648915409, 10.8415637590099, 11.1803626264790,\n+                   11.5191614939481, 11.8579603614171, 12.1967592288862,\n+                   12.5355580963552, 12.8743569638243, 13.2131558312934,\n+                   13.5519546987624, 13.8907535662315, 14.2295524337005,\n+                   14.5683513011696, 14.9071501686387, 15.2459490361077,\n+                   15.5847479035768, 15.9235467710458, 16.2623456385149,\n+                   16.6011445059840, 16.9399433734530, 17.2787422409221,\n+                   17.6175411083911, 17.9563399758602, 18.2951388433293,\n+                   18.6339377107983, 18.9727365782674, 19.3115354457364,\n+                   19.6503343132055, 19.9891331806746, 20.3279320481436,\n+                   20.6667309156127, 21.0055297830817, 21.3443286505508,\n+                   21.6831275180199, 22.0219263854889, 22.3607252529580,\n+                   22.6995241204270, 23.0383229878961, 23.3771218553652,\n+                   23.7159207228342, 24.0547195903033, 24.3935184577724,\n+                   24.7323173252414, 25.0711161927105, 25.4099150601795,\n+                   25.7487139276486, 26.0875127951177, 26.4263116625867,\n+                   26.7651105300558, 27.1039093975248, 27.4427082649939,\n+                   27.7815071324630, 28.1203059999320, 28.4591048674011,\n+                   28.7979037348701, 29.1367026023392, 29.4755014698083,\n+                   29.8143003372773, 30.1530992047464, 30.4918980722154,\n+                   30.8306969396845, 31.1694958071536, 31.5082946746226,\n+                   31.8470935420917, 32.1858924095607, 32.5246912770298,\n+                   32.8634901444989, 33.2022890119679, 33.5410878794370])\n+    i1 = np.array([3.39430882774470, 2.80864492110761, 3.28358165429196,\n+                   3.41191190551673, 3.11975662808148, 3.35436585834612,\n+                   3.23953272899809, 3.60307083325333, 2.80478101508277,\n+                   2.80505102853845, 3.16918996870373, 3.21088388439857,\n+                   3.46332865310431, 3.09224155015883, 3.17541550741062,\n+                   3.32470179290389, 3.33224664316240, 3.07709000050741,\n+                   2.89141245343405, 3.01365768561537, 3.23265176770231,\n+                   3.32253647634228, 2.97900657569736, 3.31959549243966,\n+                   3.03375461550111, 2.97579298978937, 3.25432831375159,\n+                   2.89178382564454, 3.00341909207567, 3.72637492250097,\n+                   3.28379856976360, 2.96516169245835, 3.25658381110230,\n+                   3.41655911533139, 3.02718097944604, 3.11458376760376,\n+                   3.24617304369762, 3.45935502367636, 3.21557333256913,\n+                   3.27611176482650, 2.86954135732485, 3.32416319254657,\n+                   3.15277467598732, 3.08272557013770, 3.15602202666259,\n+                   3.49432799877150, 3.53863997177632, 3.10602611478455,\n+                   3.05373911151821, 3.09876772570781, 2.97417228624287,\n+                   2.84573593699237, 3.16288578405195, 3.06533173612783,\n+                   3.02118336639575, 3.34374977225502, 2.97255164138821,\n+                   3.19286135682863, 3.10999753817133, 3.26925354620079,\n+                   3.11957809501529, 3.20155017481720, 3.31724984405837,\n+                   3.42879043512927, 3.17933067619240, 3.47777362613969,\n+                   3.20708912539777, 3.48205761174907, 3.16804363684327,\n+                   3.14055472378230, 3.13445657434470, 2.91152696252998,\n+                   3.10984113847427, 2.80443349399489, 3.23146278164875,\n+                   2.94521083406108, 3.17388903141715, 3.05930294897030,\n+                   3.18985234673287, 3.27946609274898, 3.33717523113602,\n+                   2.76394303462702, 3.19375132937510, 2.82628616689450,\n+                   2.85238527394143, 2.82975892599489, 2.79196912313914,\n+                   2.72860792049395, 2.75585977414140, 2.44280222448805,\n+                   2.36052347370628, 2.26785071765738, 2.10868255743462,\n+                   2.06165739407987, 1.90047259509385, 1.39925575828709,\n+                   1.24749015957606, 0.867823806536762, 0.432752457749993, 0])\n+    # v2, i2 produces a bad value for I0_vmp\n+    v2 = np.array([0, 0.365686097622586, 0.731372195245173, 1.09705829286776,\n+                   1.46274439049035, 1.82843048811293, 2.19411658573552,\n+                   2.55980268335810, 2.92548878098069, 3.29117487860328,\n+                   3.65686097622586, 4.02254707384845, 4.38823317147104,\n+                   4.75391926909362, 5.11960536671621, 5.48529146433880,\n+                   5.85097756196138, 6.21666365958397, 6.58234975720655,\n+                   6.94803585482914, 7.31372195245173, 7.67940805007431,\n+                   8.04509414769690, 8.41078024531949, 8.77646634294207,\n+                   9.14215244056466, 9.50783853818725, 9.87352463580983,\n+                   10.2392107334324, 10.6048968310550, 10.9705829286776,\n+                   11.3362690263002, 11.7019551239228, 12.0676412215454,\n+                   12.4333273191679, 12.7990134167905, 13.1646995144131,\n+                   13.5303856120357, 13.8960717096583, 14.2617578072809,\n+                   14.6274439049035, 14.9931300025260, 15.3588161001486,\n+                   15.7245021977712, 16.0901882953938, 16.4558743930164,\n+                   16.8215604906390, 17.1872465882616, 17.5529326858841,\n+                   17.9186187835067, 18.2843048811293, 18.6499909787519,\n+                   19.0156770763745, 19.3813631739971, 19.7470492716197,\n+                   20.1127353692422, 20.4784214668648, 20.8441075644874,\n+                   21.2097936621100, 21.5754797597326, 21.9411658573552,\n+                   22.3068519549778, 22.6725380526004, 23.0382241502229,\n+                   23.4039102478455, 23.7695963454681, 24.1352824430907,\n+                   24.5009685407133, 24.8666546383359, 25.2323407359585,\n+                   25.5980268335810, 25.9637129312036, 26.3293990288262,\n+                   26.6950851264488, 27.0607712240714, 27.4264573216940,\n+                   27.7921434193166, 28.1578295169392, 28.5235156145617,\n+                   28.8892017121843, 29.2548878098069, 29.6205739074295,\n+                   29.9862600050521, 30.3519461026747, 30.7176322002973,\n+                   31.0833182979198, 31.4490043955424, 31.8146904931650,\n+                   32.1803765907876, 32.5460626884102, 32.9117487860328,\n+                   33.2774348836554, 33.6431209812779, 34.0088070789005,\n+                   34.3744931765231, 34.7401792741457, 35.1058653717683,\n+                   35.4715514693909, 35.8372375670135, 36.2029236646360])\n+    i2 = np.array([6.49218806928330, 6.49139336899548, 6.17810697175204,\n+                   6.75197816263663, 6.59529074137515, 6.18164578868300,\n+                   6.38709397931910, 6.30685422248427, 6.44640615548925,\n+                   6.88727230397772, 6.42074852785591, 6.46348580823746,\n+                   6.38642309763941, 5.66356277572311, 6.61010381702082,\n+                   6.33288284311125, 6.22475343933610, 6.30651399433833,\n+                   6.44435022944051, 6.43741711131908, 6.03536180208946,\n+                   6.23814639328170, 5.97229140403242, 6.20790000748341,\n+                   6.22933550182341, 6.22992127804882, 6.13400871899299,\n+                   6.83491312449950, 6.07952797245846, 6.35837746415450,\n+                   6.41972128662324, 6.85256717258275, 6.25807797296759,\n+                   6.25124948151766, 6.22229212812413, 6.72249444167406,\n+                   6.41085549981649, 6.75792874870056, 6.22096181559171,\n+                   6.47839564388996, 6.56010208597432, 6.63300966556949,\n+                   6.34617546039339, 6.79812221146153, 6.14486056194136,\n+                   6.14979256889311, 6.16883037644880, 6.57309183229605,\n+                   6.40064681038509, 6.18861448239873, 6.91340138179698,\n+                   5.94164388433788, 6.23638991745862, 6.31898940411710,\n+                   6.45247884556830, 6.58081455524297, 6.64915284801713,\n+                   6.07122119270245, 6.41398258148256, 6.62144271089614,\n+                   6.36377197712687, 6.51487678829345, 6.53418950147730,\n+                   6.18886469125371, 6.26341063475750, 6.83488211680259,\n+                   6.62699397226695, 6.41286837534735, 6.44060085001851,\n+                   6.48114130629288, 6.18607038456406, 6.16923370572396,\n+                   6.64223126283631, 6.07231852289266, 5.79043710204375,\n+                   6.48463886529882, 6.36263392044401, 6.11212476454494,\n+                   6.14573900812925, 6.12568047243240, 6.43836230231577,\n+                   6.02505694060219, 6.13819468942244, 6.22100593815064,\n+                   6.02394682666345, 5.89016573063789, 5.74448527739202,\n+                   5.50415294280017, 5.31883018164157, 4.87476769510305,\n+                   4.74386713755523, 4.60638346931628, 4.06177345572680,\n+                   3.73334482123538, 3.13848311672243, 2.71638862600768,\n+                   2.02963773590165, 1.49291145092070, 0.818343889647352, 0])\n+\n+    return v1, i1, v2, i2\n",
        "problem_statement": "CEC 6-parameter coefficient generation\nSAM is able to extract the CEC parameters required for calcparams_desoto.  This is done through the 'CEC Performance Model with User Entered Specifications' module model, and coefficients are automatically extracted given nameplate parameters Voc, Isc, Imp, Vmp and TempCoeff.  The method is based on Aron Dobos' \"An Improved Coefficient Calculator for the California Energy Commission 6 Parameter Photovoltaic Module Model \", 2012\r\n\r\nIdeally we should be able to work with the SAM open source code, extract the bit that does the coefficient generation, and put it into a PVLib function that would allow users to run calcparams_desoto with any arbitrary module type.  At the moment we are dependent on PV modules loaded into the SAM or CEC database.\r\n\r\nThank you!\r\n\n",
        "hints_text": "SAM solution routine is located at https://github.com/NREL/ssc/blob/develop/shared/6par_solve.h , function \"solve_with_sanity_and_heuristics\" . Additional dependencies on other files 6par_newton.h, 6par_search.h, 6par_jacobian.h, 6par_lu.h, 6par_gamma.h, all located in https://github.com/NREL/ssc/tree/develop/shared . \nI'd like to try to take this up sometime soon, maybe by the end of the year? Is anyone else working on it? I haven't seen a PR. Does anyone have any strong opinions? Is seems like there's a SAM implementation, is there a PVLIB-MATLAB version already? thx\nI'm not working on it. If it is practical, it would be great to wrap the SAM method. NREL put a lot of effort into the heuristics for initial values and updating.  We have a function in PVLib for MATLAB but it's a very different algorithm. For consistency with the parameter databases it would be better to use the SAM algorithm.\nDo I read between the lines that the two algorithms do not converge on the same parameters?  Then it would be of great interest to have python implementations of each!  Or perhaps even more important: documents describing the full algorithms along with every heuristic, assumption and constraint they implement...\n@adriesse yes. Each model fitting algorithm I'm aware of, will produce different parameter values from the same data although the predicted IV curves may be similar.\nFWIW I am trying to establish a common/fair metric to compare different model fits in PVfit. In addition to choosing, say, which residual(s) to analyze, the effect of model discrepancy significantly affects such comparisons. PVfit\u2019s orthogonal distance regression (ODR) fits to the single-diode model (SDM) often are worse than other algorithms\u2019 fits in terms of terminal current residuals, but I have observed consistently that this is likely due to model discrepancy. One might claim that ODR is being more \u201chonest\u201d here, but I think it really means that the model should be improved. Note that PVfit\u2019s ODR alternatively optimizes the residual of the sum of currents at the high-voltage diode node in the equivalent-circuit model, and it doesn\u2019t make unfounded assumptions about error-free measurements in all data channels except the terminal current (which also has implications for parameter non-identifiability). I have seen plenty of evidence that ODR fits are \u201cgood\u201d in the absence of significant model discrepancy, such as for some (but not all!) double-diode model (DDM) fits across various material systems. \n> If it is practical, it would be great to wrap the SAM method. \r\n\r\nTwo kinds of practicalities here:\r\n\r\n1. Technical. I don't know how to do it but I am sure it's possible. I am ok with it so long as pvlib remains straightforward to install from source.\r\n2. Legal. We would need assurance from NREL that it's ok for pvlib to distribute this SAM code under the terms of a MIT license rather than SAM's mixed MIT/GPL license.\r\n\nIn the next 6-8 months, the NREL team will be updating our python wrapper for the SAM software development kit so that you can pip install the sam-sdk and call its routines in a much more native pythonic fashion. That might be a quick way to implement the CEC parameter generation to solve both the technical and legal practicalities Will mentions above, as well as the many file dependencies involved with that routine. The SDK is licensed separately from the SAM open source code under an MIT type license, so that distribution with pvlib would be ok, and once we have it set up as a python package, that should make it a non-issue to include with pvlib installation from source. \r\n\r\nTo @adriesse 's point, the publication associated with the method implemented in SAM is here: http://solarenergyengineering.asmedigitalcollection.asme.org/article.aspx?articleid=1458865 \nThanks @janinefreeman that is great news!\n@janinefreeman Does NREL's web site no longer provide free preprints of Aron's paper?\n@thunderfish24  I'm not finding a copy of it in NREL's pubs database, but it looks like you may be able to get it here (didn't create an account to give it a try): https://www.osti.gov/biblio/1043759 \n@thunderfish24 I sent you a preprint.",
        "created_at": "2019-05-13T14:43:11Z",
        "version": "0.5",
        "FAIL_TO_PASS": [
            "pvlib/test/test_ivtools.py::test_fit_sde_sandia",
            "pvlib/test/test_ivtools.py::test_fit_sde_sandia_bad_iv"
        ],
        "PASS_TO_PASS": [],
        "environment_setup_commit": "84818c6d950142927359ffe308c728a0c080ddce"
    },
    {
        "repo": "pylint-dev/astroid",
        "instance_id": "pylint-dev__astroid-1364",
        "base_commit": "9363c34934f94124f4867caf1bdf8f6755201ccd",
        "patch": "diff --git a/astroid/const.py b/astroid/const.py\n--- a/astroid/const.py\n+++ b/astroid/const.py\n@@ -8,6 +8,8 @@\n PY310_PLUS = sys.version_info >= (3, 10)\n BUILTINS = \"builtins\"  # TODO Remove in 2.8\n \n+WIN32 = sys.platform == \"win32\"\n+\n \n class Context(enum.Enum):\n     Load = 1\n",
        "test_patch": "diff --git a/tests/unittest_scoped_nodes.py b/tests/unittest_scoped_nodes.py\n--- a/tests/unittest_scoped_nodes.py\n+++ b/tests/unittest_scoped_nodes.py\n@@ -43,9 +43,9 @@\n \n import pytest\n \n-from astroid import MANAGER, builder, nodes, objects, test_utils, util\n+from astroid import MANAGER, builder, nodes, objects, parse, test_utils, util\n from astroid.bases import BoundMethod, Generator, Instance, UnboundMethod\n-from astroid.const import PY38_PLUS\n+from astroid.const import PY38_PLUS, PY310_PLUS, WIN32\n from astroid.exceptions import (\n     AttributeInferenceError,\n     DuplicateBasesError,\n@@ -1670,6 +1670,49 @@ class B(A[T], A[T]): ...\n         with self.assertRaises(DuplicateBasesError):\n             cls.mro()\n \n+    @test_utils.require_version(minver=\"3.7\")\n+    def test_mro_typing_extensions(self):\n+        \"\"\"Regression test for mro() inference on typing_extesnions.\n+\n+        Regression reported in:\n+        https://github.com/PyCQA/astroid/issues/1124\n+        \"\"\"\n+        module = parse(\n+            \"\"\"\n+        import abc\n+        import typing\n+        import dataclasses\n+\n+        import typing_extensions\n+\n+        T = typing.TypeVar(\"T\")\n+\n+        class MyProtocol(typing_extensions.Protocol): pass\n+        class EarlyBase(typing.Generic[T], MyProtocol): pass\n+        class Base(EarlyBase[T], abc.ABC): pass\n+        class Final(Base[object]): pass\n+        \"\"\"\n+        )\n+        class_names = [\n+            \"ABC\",\n+            \"Base\",\n+            \"EarlyBase\",\n+            \"Final\",\n+            \"Generic\",\n+            \"MyProtocol\",\n+            \"Protocol\",\n+            \"object\",\n+        ]\n+        if not PY38_PLUS:\n+            class_names.pop(-2)\n+        # typing_extensions is not installed on this combination of version\n+        # and platform\n+        if PY310_PLUS and WIN32:\n+            class_names.pop(-2)\n+\n+        final_def = module.body[-1]\n+        self.assertEqual(class_names, sorted(i.name for i in final_def.mro()))\n+\n     def test_generator_from_infer_call_result_parent(self) -> None:\n         func = builder.extract_node(\n             \"\"\"\n",
        "problem_statement": "MRO failure on Python 3.7 with typing_extensions\n### Steps to reproduce\r\n\r\nRun the following script on Python 3.7:\r\n\r\n```python\r\nfrom astroid import parse\r\nmodule = parse(\"\"\"\r\nimport abc\r\nimport typing\r\nimport dataclasses\r\n\r\nimport typing_extensions\r\n\r\nT = typing.TypeVar(\"T\")\r\n\r\nclass MyProtocol(typing_extensions.Protocol): pass\r\nclass EarlyBase(typing.Generic[T], MyProtocol): pass\r\nclass Base(EarlyBase[T], abc.ABC): pass\r\nclass Final(Base[object]): pass\r\n\"\"\")\r\n\r\n#                    typing.Protocol\r\n#                          |\r\n# typing.Generic[T]    MyProtocol\r\n#              \\       /\r\n#              EarlyBase     abc.ABC\r\n#                       \\    /\r\n#                        Base\r\n#                         |\r\n#                        Final\r\n\r\nfinal_def = module.body[-1]\r\nfinal_def.mro()\r\n```\r\n\r\n### Current behavior\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"xxx.py\", line 31, in <module>\r\n    print(\"mro:\", final_def.mro())\r\n  File \"/home/rturner/astroid/astroid/nodes/scoped_nodes.py\", line 3009, in mro\r\n    return self._compute_mro(context=context)\r\n  File \"/home/rturner/astroid/astroid/nodes/scoped_nodes.py\", line 2985, in _compute_mro\r\n    mro = base._compute_mro(context=context)\r\n  File \"/home/rturner/astroid/astroid/nodes/scoped_nodes.py\", line 2999, in _compute_mro\r\n    return _c3_merge(unmerged_mro, self, context)\r\n  File \"/home/rturner/astroid/astroid/nodes/scoped_nodes.py\", line 103, in _c3_merge\r\n    context=context,\r\nastroid.exceptions.InconsistentMroError: Cannot create a consistent method resolution order for MROs (tuple, object), (EarlyBase, tuple, Generic, object, MyProtocol), (ABC, object), (tuple, EarlyBase, ABC) of class <ClassDef.Base l.1347 at 0x7fa0efd52590>.\r\n```\r\n\r\n### Expected behavior\r\n\r\nNo MRO error is raised; Python 3.7 doesn't raise an error.\r\n\r\n### `python -c \"from astroid import __pkginfo__; print(__pkginfo__.version)\"` output\r\n\r\n2.6.7-dev0; the test case fails in pylint 2.9.6 and on the main branch at commit 6e8699cef0888631bd827b096533fc6e894d2fb2.\n",
        "hints_text": "",
        "created_at": "2022-01-20T10:00:25Z",
        "version": "2.10",
        "FAIL_TO_PASS": [
            "tests/unittest_scoped_nodes.py::ModuleNodeTest::test_dict_interface",
            "tests/unittest_scoped_nodes.py::ModuleNodeTest::test_file_stream_api",
            "tests/unittest_scoped_nodes.py::ModuleNodeTest::test_file_stream_in_memory",
            "tests/unittest_scoped_nodes.py::ModuleNodeTest::test_file_stream_physical",
            "tests/unittest_scoped_nodes.py::ModuleNodeTest::test_getattr",
            "tests/unittest_scoped_nodes.py::ModuleNodeTest::test_import_1",
            "tests/unittest_scoped_nodes.py::ModuleNodeTest::test_import_2",
            "tests/unittest_scoped_nodes.py::ModuleNodeTest::test_module_getattr",
            "tests/unittest_scoped_nodes.py::ModuleNodeTest::test_public_names",
            "tests/unittest_scoped_nodes.py::ModuleNodeTest::test_relative_to_absolute_name",
            "tests/unittest_scoped_nodes.py::ModuleNodeTest::test_relative_to_absolute_name_beyond_top_level",
            "tests/unittest_scoped_nodes.py::ModuleNodeTest::test_special_attributes",
            "tests/unittest_scoped_nodes.py::ModuleNodeTest::test_stream_api",
            "tests/unittest_scoped_nodes.py::ModuleNodeTest::test_wildcard_import_names",
            "tests/unittest_scoped_nodes.py::FunctionNodeTest::test_abstract_methods_are_not_implicitly_none",
            "tests/unittest_scoped_nodes.py::FunctionNodeTest::test_argnames",
            "tests/unittest_scoped_nodes.py::FunctionNodeTest::test_decorator_builtin_descriptors",
            "tests/unittest_scoped_nodes.py::FunctionNodeTest::test_default_value",
            "tests/unittest_scoped_nodes.py::FunctionNodeTest::test_dict_interface",
            "tests/unittest_scoped_nodes.py::FunctionNodeTest::test_dunder_class_local_to_classmethod",
            "tests/unittest_scoped_nodes.py::FunctionNodeTest::test_dunder_class_local_to_function",
            "tests/unittest_scoped_nodes.py::FunctionNodeTest::test_dunder_class_local_to_method",
            "tests/unittest_scoped_nodes.py::FunctionNodeTest::test_format_args",
            "tests/unittest_scoped_nodes.py::FunctionNodeTest::test_format_args_keyword_only_args",
            "tests/unittest_scoped_nodes.py::FunctionNodeTest::test_four_args",
            "tests/unittest_scoped_nodes.py::FunctionNodeTest::test_func_instance_attr",
            "tests/unittest_scoped_nodes.py::FunctionNodeTest::test_igetattr",
            "tests/unittest_scoped_nodes.py::FunctionNodeTest::test_is_abstract",
            "tests/unittest_scoped_nodes.py::FunctionNodeTest::test_is_abstract_decorated",
            "tests/unittest_scoped_nodes.py::FunctionNodeTest::test_is_generator",
            "tests/unittest_scoped_nodes.py::FunctionNodeTest::test_is_method",
            "tests/unittest_scoped_nodes.py::FunctionNodeTest::test_lambda_pytype",
            "tests/unittest_scoped_nodes.py::FunctionNodeTest::test_lambda_qname",
            "tests/unittest_scoped_nodes.py::FunctionNodeTest::test_method_init_subclass",
            "tests/unittest_scoped_nodes.py::FunctionNodeTest::test_navigation",
            "tests/unittest_scoped_nodes.py::FunctionNodeTest::test_no_returns_is_implicitly_none",
            "tests/unittest_scoped_nodes.py::FunctionNodeTest::test_only_raises_is_not_implicitly_none",
            "tests/unittest_scoped_nodes.py::FunctionNodeTest::test_return_annotation_is_not_the_last",
            "tests/unittest_scoped_nodes.py::FunctionNodeTest::test_return_nothing",
            "tests/unittest_scoped_nodes.py::FunctionNodeTest::test_special_attributes",
            "tests/unittest_scoped_nodes.py::FunctionNodeTest::test_type_builtin_descriptor_subclasses",
            "tests/unittest_scoped_nodes.py::ClassNodeTest::test__bases__attribute",
            "tests/unittest_scoped_nodes.py::ClassNodeTest::test__mro__attribute",
            "tests/unittest_scoped_nodes.py::ClassNodeTest::test_add_metaclass",
            "tests/unittest_scoped_nodes.py::ClassNodeTest::test_all_ancestors_need_slots",
            "tests/unittest_scoped_nodes.py::ClassNodeTest::test_ancestors",
            "tests/unittest_scoped_nodes.py::ClassNodeTest::test_class_extra_decorators",
            "tests/unittest_scoped_nodes.py::ClassNodeTest::test_class_extra_decorators_frame_is_not_class",
            "tests/unittest_scoped_nodes.py::ClassNodeTest::test_class_extra_decorators_only_assignment_names_are_considered",
            "tests/unittest_scoped_nodes.py::ClassNodeTest::test_class_extra_decorators_only_callfunc_are_considered",
            "tests/unittest_scoped_nodes.py::ClassNodeTest::test_class_extra_decorators_only_same_name_considered",
            "tests/unittest_scoped_nodes.py::ClassNodeTest::test_class_getattr",
            "tests/unittest_scoped_nodes.py::ClassNodeTest::test_class_keywords",
            "tests/unittest_scoped_nodes.py::ClassNodeTest::test_classmethod_attributes",
            "tests/unittest_scoped_nodes.py::ClassNodeTest::test_cls_special_attributes_1",
            "tests/unittest_scoped_nodes.py::ClassNodeTest::test_cls_special_attributes_2",
            "tests/unittest_scoped_nodes.py::ClassNodeTest::test_dict_interface",
            "tests/unittest_scoped_nodes.py::ClassNodeTest::test_duplicate_bases_namedtuple",
            "tests/unittest_scoped_nodes.py::ClassNodeTest::test_extra_decorators_only_class_level_assignments",
            "tests/unittest_scoped_nodes.py::ClassNodeTest::test_function_with_decorator_lineno",
            "tests/unittest_scoped_nodes.py::ClassNodeTest::test_generator_from_infer_call_result_parent",
            "tests/unittest_scoped_nodes.py::ClassNodeTest::test_getattr_from_grandpa",
            "tests/unittest_scoped_nodes.py::ClassNodeTest::test_getattr_method_transform",
            "tests/unittest_scoped_nodes.py::ClassNodeTest::test_has_dynamic_getattr",
            "tests/unittest_scoped_nodes.py::ClassNodeTest::test_implicit_metaclass",
            "tests/unittest_scoped_nodes.py::ClassNodeTest::test_implicit_metaclass_lookup",
            "tests/unittest_scoped_nodes.py::ClassNodeTest::test_inner_classes",
            "tests/unittest_scoped_nodes.py::ClassNodeTest::test_instance_attr_ancestors",
            "tests/unittest_scoped_nodes.py::ClassNodeTest::test_instance_bound_method_lambdas",
            "tests/unittest_scoped_nodes.py::ClassNodeTest::test_instance_bound_method_lambdas_2",
            "tests/unittest_scoped_nodes.py::ClassNodeTest::test_instance_getattr",
            "tests/unittest_scoped_nodes.py::ClassNodeTest::test_instance_getattr_with_class_attr",
            "tests/unittest_scoped_nodes.py::ClassNodeTest::test_instance_special_attributes",
            "tests/unittest_scoped_nodes.py::ClassNodeTest::test_kite_graph",
            "tests/unittest_scoped_nodes.py::ClassNodeTest::test_local_attr_ancestors",
            "tests/unittest_scoped_nodes.py::ClassNodeTest::test_local_attr_invalid_mro",
            "tests/unittest_scoped_nodes.py::ClassNodeTest::test_local_attr_mro",
            "tests/unittest_scoped_nodes.py::ClassNodeTest::test_metaclass_error",
            "tests/unittest_scoped_nodes.py::ClassNodeTest::test_metaclass_lookup",
            "tests/unittest_scoped_nodes.py::ClassNodeTest::test_metaclass_lookup_inference_errors",
            "tests/unittest_scoped_nodes.py::ClassNodeTest::test_metaclass_lookup_using_same_class",
            "tests/unittest_scoped_nodes.py::ClassNodeTest::test_metaclass_type",
            "tests/unittest_scoped_nodes.py::ClassNodeTest::test_metaclass_yes_leak",
            "tests/unittest_scoped_nodes.py::ClassNodeTest::test_methods",
            "tests/unittest_scoped_nodes.py::ClassNodeTest::test_mro",
            "tests/unittest_scoped_nodes.py::ClassNodeTest::test_mro_generic_1",
            "tests/unittest_scoped_nodes.py::ClassNodeTest::test_mro_generic_2",
            "tests/unittest_scoped_nodes.py::ClassNodeTest::test_mro_generic_3",
            "tests/unittest_scoped_nodes.py::ClassNodeTest::test_mro_generic_4",
            "tests/unittest_scoped_nodes.py::ClassNodeTest::test_mro_generic_5",
            "tests/unittest_scoped_nodes.py::ClassNodeTest::test_mro_generic_6",
            "tests/unittest_scoped_nodes.py::ClassNodeTest::test_mro_generic_7",
            "tests/unittest_scoped_nodes.py::ClassNodeTest::test_mro_generic_error_1",
            "tests/unittest_scoped_nodes.py::ClassNodeTest::test_mro_generic_error_2",
            "tests/unittest_scoped_nodes.py::ClassNodeTest::test_mro_typing_extensions",
            "tests/unittest_scoped_nodes.py::ClassNodeTest::test_mro_with_attribute_classes",
            "tests/unittest_scoped_nodes.py::ClassNodeTest::test_mro_with_factories",
            "tests/unittest_scoped_nodes.py::ClassNodeTest::test_navigation",
            "tests/unittest_scoped_nodes.py::ClassNodeTest::test_no_infinite_metaclass_loop",
            "tests/unittest_scoped_nodes.py::ClassNodeTest::test_no_infinite_metaclass_loop_with_redefine",
            "tests/unittest_scoped_nodes.py::ClassNodeTest::test_nonregr_infer_callresult",
            "tests/unittest_scoped_nodes.py::ClassNodeTest::test_slots",
            "tests/unittest_scoped_nodes.py::ClassNodeTest::test_slots_added_dynamically_still_inferred",
            "tests/unittest_scoped_nodes.py::ClassNodeTest::test_slots_empty_list_of_slots",
            "tests/unittest_scoped_nodes.py::ClassNodeTest::test_slots_for_dict_keys",
            "tests/unittest_scoped_nodes.py::ClassNodeTest::test_slots_taken_from_parents",
            "tests/unittest_scoped_nodes.py::ClassNodeTest::test_type",
            "tests/unittest_scoped_nodes.py::ClassNodeTest::test_type_three_arguments",
            "tests/unittest_scoped_nodes.py::test_issue940_metaclass_subclass_property",
            "tests/unittest_scoped_nodes.py::test_issue940_property_grandchild",
            "tests/unittest_scoped_nodes.py::test_issue940_metaclass_property",
            "tests/unittest_scoped_nodes.py::test_issue940_with_metaclass_class_context_property",
            "tests/unittest_scoped_nodes.py::test_issue940_metaclass_values_funcdef",
            "tests/unittest_scoped_nodes.py::test_issue940_metaclass_derived_funcdef",
            "tests/unittest_scoped_nodes.py::test_issue940_metaclass_funcdef_is_not_datadescriptor",
            "tests/unittest_scoped_nodes.py::test_issue940_enums_as_a_real_world_usecase",
            "tests/unittest_scoped_nodes.py::test_metaclass_cannot_infer_call_yields_an_instance",
            "tests/unittest_scoped_nodes.py::test_posonlyargs_python_38[\\ndef",
            "tests/unittest_scoped_nodes.py::test_posonlyargs_default_value",
            "tests/unittest_scoped_nodes.py::test_ancestor_with_generic",
            "tests/unittest_scoped_nodes.py::test_slots_duplicate_bases_issue_1089",
            "tests/unittest_scoped_nodes.py::TestFrameNodes::test_frame_node",
            "tests/unittest_scoped_nodes.py::TestFrameNodes::test_non_frame_node"
        ],
        "PASS_TO_PASS": [],
        "environment_setup_commit": "da745538c7236028a22cdf0405f6829fcf6886bc"
    },
    {
        "repo": "pydicom/pydicom",
        "instance_id": "pydicom__pydicom-901",
        "base_commit": "3746878d8edf1cbda6fbcf35eec69f9ba79301ca",
        "patch": "diff --git a/pydicom/config.py b/pydicom/config.py\n--- a/pydicom/config.py\n+++ b/pydicom/config.py\n@@ -62,10 +62,7 @@ def DS_decimal(use_Decimal_boolean=True):\n \n # Logging system and debug function to change logging level\n logger = logging.getLogger('pydicom')\n-handler = logging.StreamHandler()\n-formatter = logging.Formatter(\"%(message)s\")\n-handler.setFormatter(formatter)\n-logger.addHandler(handler)\n+logger.addHandler(logging.NullHandler())\n \n \n import pydicom.pixel_data_handlers.numpy_handler as np_handler  # noqa\n@@ -110,16 +107,29 @@ def get_pixeldata(ds):\n \"\"\"\n \n \n-def debug(debug_on=True):\n-    \"\"\"Turn debugging of DICOM file reading and writing on or off.\n+def debug(debug_on=True, default_handler=True):\n+    \"\"\"Turn on/off debugging of DICOM file reading and writing.\n+\n     When debugging is on, file location and details about the\n     elements read at that location are logged to the 'pydicom'\n     logger using python's logging module.\n \n-    :param debug_on: True (default) to turn on debugging,\n-    False to turn off.\n+    Parameters\n+    ----------\n+    debug_on : bool, optional\n+        If True (default) then turn on debugging, False to turn off.\n+    default_handler : bool, optional\n+        If True (default) then use ``logging.StreamHandler()`` as the handler\n+        for log messages.\n     \"\"\"\n     global logger, debugging\n+\n+    if default_handler:\n+        handler = logging.StreamHandler()\n+        formatter = logging.Formatter(\"%(message)s\")\n+        handler.setFormatter(formatter)\n+        logger.addHandler(handler)\n+\n     if debug_on:\n         logger.setLevel(logging.DEBUG)\n         debugging = True\n@@ -129,4 +139,4 @@ def debug(debug_on=True):\n \n \n # force level=WARNING, in case logging default is set differently (issue 103)\n-debug(False)\n+debug(False, False)\n",
        "test_patch": "diff --git a/pydicom/tests/test_config.py b/pydicom/tests/test_config.py\nnew file mode 100644\n--- /dev/null\n+++ b/pydicom/tests/test_config.py\n@@ -0,0 +1,107 @@\n+# Copyright 2008-2019 pydicom authors. See LICENSE file for details.\n+\"\"\"Unit tests for the pydicom.config module.\"\"\"\n+\n+import logging\n+import sys\n+\n+import pytest\n+\n+from pydicom import dcmread\n+from pydicom.config import debug\n+from pydicom.data import get_testdata_files\n+\n+\n+DS_PATH = get_testdata_files(\"CT_small.dcm\")[0]\n+PYTEST = [int(x) for x in pytest.__version__.split('.')]\n+\n+\n+@pytest.mark.skipif(PYTEST[:2] < [3, 4], reason='no caplog')\n+class TestDebug(object):\n+    \"\"\"Tests for config.debug().\"\"\"\n+    def setup(self):\n+        self.logger = logging.getLogger('pydicom')\n+\n+    def teardown(self):\n+        # Reset to just NullHandler\n+        self.logger.handlers = [self.logger.handlers[0]]\n+\n+    def test_default(self, caplog):\n+        \"\"\"Test that the default logging handler is a NullHandler.\"\"\"\n+        assert 1 == len(self.logger.handlers)\n+        assert isinstance(self.logger.handlers[0], logging.NullHandler)\n+\n+        with caplog.at_level(logging.DEBUG, logger='pydicom'):\n+            ds = dcmread(DS_PATH)\n+\n+            assert \"Call to dcmread()\" not in caplog.text\n+            assert \"Reading File Meta Information preamble...\" in caplog.text\n+            assert \"Reading File Meta Information prefix...\" in caplog.text\n+            assert \"00000080: 'DICM' prefix found\" in caplog.text\n+\n+    def test_debug_on_handler_null(self, caplog):\n+        \"\"\"Test debug(True, False).\"\"\"\n+        debug(True, False)\n+        assert 1 == len(self.logger.handlers)\n+        assert isinstance(self.logger.handlers[0], logging.NullHandler)\n+\n+        with caplog.at_level(logging.DEBUG, logger='pydicom'):\n+            ds = dcmread(DS_PATH)\n+\n+            assert \"Call to dcmread()\" in caplog.text\n+            assert \"Reading File Meta Information preamble...\" in caplog.text\n+            assert \"Reading File Meta Information prefix...\" in caplog.text\n+            assert \"00000080: 'DICM' prefix found\" in caplog.text\n+            msg = (\n+                \"00009848: fc ff fc ff 4f 42 00 00 7e 00 00 00    \"\n+                \"(fffc, fffc) OB Length: 126\"\n+            )\n+            assert msg in caplog.text\n+\n+    def test_debug_off_handler_null(self, caplog):\n+        \"\"\"Test debug(False, False).\"\"\"\n+        debug(False, False)\n+        assert 1 == len(self.logger.handlers)\n+        assert isinstance(self.logger.handlers[0], logging.NullHandler)\n+\n+        with caplog.at_level(logging.DEBUG, logger='pydicom'):\n+            ds = dcmread(DS_PATH)\n+\n+            assert \"Call to dcmread()\" not in caplog.text\n+            assert \"Reading File Meta Information preamble...\" in caplog.text\n+            assert \"Reading File Meta Information prefix...\" in caplog.text\n+            assert \"00000080: 'DICM' prefix found\" in caplog.text\n+\n+    def test_debug_on_handler_stream(self, caplog):\n+        \"\"\"Test debug(True, True).\"\"\"\n+        debug(True, True)\n+        assert 2 == len(self.logger.handlers)\n+        assert isinstance(self.logger.handlers[0], logging.NullHandler)\n+        assert isinstance(self.logger.handlers[1], logging.StreamHandler)\n+\n+        with caplog.at_level(logging.DEBUG, logger='pydicom'):\n+            ds = dcmread(DS_PATH)\n+\n+            assert \"Call to dcmread()\" in caplog.text\n+            assert \"Reading File Meta Information preamble...\" in caplog.text\n+            assert \"Reading File Meta Information prefix...\" in caplog.text\n+            assert \"00000080: 'DICM' prefix found\" in caplog.text\n+            msg = (\n+                \"00009848: fc ff fc ff 4f 42 00 00 7e 00 00 00    \"\n+                \"(fffc, fffc) OB Length: 126\"\n+            )\n+            assert msg in caplog.text\n+\n+    def test_debug_off_handler_stream(self, caplog):\n+        \"\"\"Test debug(False, True).\"\"\"\n+        debug(False, True)\n+        assert 2 == len(self.logger.handlers)\n+        assert isinstance(self.logger.handlers[0], logging.NullHandler)\n+        assert isinstance(self.logger.handlers[1], logging.StreamHandler)\n+\n+        with caplog.at_level(logging.DEBUG, logger='pydicom'):\n+            ds = dcmread(DS_PATH)\n+\n+            assert \"Call to dcmread()\" not in caplog.text\n+            assert \"Reading File Meta Information preamble...\" in caplog.text\n+            assert \"Reading File Meta Information prefix...\" in caplog.text\n+            assert \"00000080: 'DICM' prefix found\" in caplog.text\n",
        "problem_statement": "pydicom should not define handler, formatter and log level.\nThe `config` module (imported when pydicom is imported) defines a handler and set the log level for the pydicom logger. This should not be the case IMO. It should be the responsibility of the client code of pydicom to configure the logging module to its convenience. Otherwise one end up having multiple logs record as soon as pydicom is imported:\r\n\r\nExample:\r\n```\r\nCould not import pillow\r\n2018-03-25 15:27:29,744 :: DEBUG :: pydicom \r\n  Could not import pillow\r\nCould not import jpeg_ls\r\n2018-03-25 15:27:29,745 :: DEBUG :: pydicom \r\n  Could not import jpeg_ls\r\nCould not import gdcm\r\n2018-03-25 15:27:29,745 :: DEBUG :: pydicom \r\n  Could not import gdcm\r\n``` \r\nOr am I missing something?\n",
        "hints_text": "In addition, I don't understand what the purpose of the `config.debug` function since the default behavor of the logging module in absence of configuartion seems to already be the one you want.\r\n\r\nFrom https://docs.python.org/3/howto/logging.html#configuring-logging-for-a-library:\r\n\r\n> If the using application does not use logging, and library code makes logging calls, then (as described in the previous section) events of severity WARNING and greater will be printed to sys.stderr. This is regarded as the best default behaviour.\r\n\r\nand\r\n\r\n>**It is strongly advised that you do not add any handlers other than NullHandler to your library\u2019s loggers.** This is because the configuration of handlers is the prerogative of the application developer who uses your library. The application developer knows their target audience and what handlers are most appropriate for their application: if you add handlers \u2018under the hood\u2019, you might well interfere with their ability to carry out unit tests and deliver logs which suit their requirements. \r\n\nI think you make good points here.  I support changing the logging to comply with python's suggested behavior.\r\n\r\n> In addition, I don't understand what the purpose of the config.debug function\r\n\r\nOne reason is that the core loop in pydicom (data_element_generator in filereader.py) is extremely optimized for speed - it checks the `debugging` flag set by config.debug, to avoid composing messages and doing function calls to logger when not needed.",
        "created_at": "2019-07-27T00:18:11Z",
        "version": "1.3",
        "FAIL_TO_PASS": [
            "pydicom/tests/test_config.py::TestDebug::test_default",
            "pydicom/tests/test_config.py::TestDebug::test_debug_on_handler_null",
            "pydicom/tests/test_config.py::TestDebug::test_debug_off_handler_null",
            "pydicom/tests/test_config.py::TestDebug::test_debug_on_handler_stream",
            "pydicom/tests/test_config.py::TestDebug::test_debug_off_handler_stream"
        ],
        "PASS_TO_PASS": [],
        "environment_setup_commit": "7241f5d9db0de589b230bb84212fbb643a7c86c3"
    }
]