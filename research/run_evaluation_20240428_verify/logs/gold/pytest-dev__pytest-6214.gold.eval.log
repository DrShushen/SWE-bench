[pytest-dev__pytest__5.2] [pytest-dev__pytest-6214] Task Metadata:
	- Instance ID: pytest-dev__pytest-6214
	- Testbed: /home/user/SWE-bench/research/run_evaluation_20240428_verify/testbed/gold/pytest/5.2/tmpsxg7_z0p/pytest-dev__pytest__5.2
	- Virtual Env.: pytest-dev__pytest__5.2
	- Evaluation Model: gold 
[pytest-dev__pytest__5.2] [pytest-dev__pytest-6214] Command: git ls-files --ignored --exclude-standard -o -z | xargs -0 -r rm -rf 
[pytest-dev__pytest__5.2] [pytest-dev__pytest-6214] Subprocess args: {"check": true, "shell": false, "capture_output": false, "text": true, "env": {"CONDA_PKGS_DIRS": "/home/user/SWE-bench/research/run_evaluation_20240428_verify/testbed/gold/pytest/5.2/tmpmb4ql99z/miniconda3/cache"}, "stdout": -1, "stderr": -2} 
[pytest-dev__pytest__5.2] [pytest-dev__pytest-6214] Command: git restore . 
[pytest-dev__pytest__5.2] [pytest-dev__pytest-6214] Subprocess args: {"check": true, "shell": false, "capture_output": false, "text": true, "env": {"CONDA_PKGS_DIRS": "/home/user/SWE-bench/research/run_evaluation_20240428_verify/testbed/gold/pytest/5.2/tmpmb4ql99z/miniconda3/cache"}, "stdout": -1, "stderr": -2} 
[pytest-dev__pytest__5.2] [pytest-dev__pytest-6214] Std. Output:
 
[pytest-dev__pytest__5.2] [pytest-dev__pytest-6214] Return Code: 0 
[pytest-dev__pytest__5.2] [pytest-dev__pytest-6214] Command: git reset HEAD . 
[pytest-dev__pytest__5.2] [pytest-dev__pytest-6214] Subprocess args: {"check": true, "shell": false, "capture_output": false, "text": true, "env": {"CONDA_PKGS_DIRS": "/home/user/SWE-bench/research/run_evaluation_20240428_verify/testbed/gold/pytest/5.2/tmpmb4ql99z/miniconda3/cache"}, "stdout": -1, "stderr": -2} 
[pytest-dev__pytest__5.2] [pytest-dev__pytest-6214] Std. Output:
 
[pytest-dev__pytest__5.2] [pytest-dev__pytest-6214] Return Code: 0 
[pytest-dev__pytest__5.2] [pytest-dev__pytest-6214] Command: git clean -fdx 
[pytest-dev__pytest__5.2] [pytest-dev__pytest-6214] Subprocess args: {"check": true, "shell": false, "capture_output": false, "text": true, "env": {"CONDA_PKGS_DIRS": "/home/user/SWE-bench/research/run_evaluation_20240428_verify/testbed/gold/pytest/5.2/tmpmb4ql99z/miniconda3/cache"}, "stdout": -1, "stderr": -2} 
[pytest-dev__pytest__5.2] [pytest-dev__pytest-6214] Std. Output:
 
[pytest-dev__pytest__5.2] [pytest-dev__pytest-6214] Return Code: 0 
[pytest-dev__pytest__5.2] [pytest-dev__pytest-6214] Command: git -c advice.detachedHead=false checkout f24f20a46e0efd8b375ab3457e9f6864e59979e5 
[pytest-dev__pytest__5.2] [pytest-dev__pytest-6214] Subprocess args: {"check": true, "shell": false, "capture_output": false, "text": true, "env": {"CONDA_PKGS_DIRS": "/home/user/SWE-bench/research/run_evaluation_20240428_verify/testbed/gold/pytest/5.2/tmpmb4ql99z/miniconda3/cache"}, "stdout": -1, "stderr": -2} 
[pytest-dev__pytest__5.2] [pytest-dev__pytest-6214] Std. Output:
HEAD is now at f24f20a46 Merge pull request #6204 from TH3CHARLie/add-changelog-on-tmp_path_factory
 
[pytest-dev__pytest__5.2] [pytest-dev__pytest-6214] Return Code: 0 
[pytest-dev__pytest__5.2] [pytest-dev__pytest-6214] Reset task environment to f24f20a46e0efd8b375ab3457e9f6864e59979e5 
[pytest-dev__pytest__5.2] [pytest-dev__pytest-6214] Command: git apply -v /home/user/SWE-bench/research/run_evaluation_20240428_verify/testbed/gold/pytest/5.2/tmpsxg7_z0p/temp_pytest-dev__pytest-6214_pred_try.patch 
[pytest-dev__pytest__5.2] [pytest-dev__pytest-6214] Subprocess args: {"check": false, "shell": false, "capture_output": false, "text": true, "env": {"CONDA_PKGS_DIRS": "/home/user/SWE-bench/research/run_evaluation_20240428_verify/testbed/gold/pytest/5.2/tmpmb4ql99z/miniconda3/cache"}, "stdout": -1, "stderr": -2} 
[pytest-dev__pytest__5.2] [pytest-dev__pytest-6214] Std. Output:
Checking patch src/_pytest/setupplan.py...
Applied patch src/_pytest/setupplan.py cleanly.
 
[pytest-dev__pytest__5.2] [pytest-dev__pytest-6214] Return Code: 0 
[pytest-dev__pytest__5.2] [pytest-dev__pytest-6214] Apply patch successful (pred_try) 
>>>>> Applied Patch (pred_try)
[pytest-dev__pytest__5.2] [pytest-dev__pytest-6214] Command: git apply -v -R /home/user/SWE-bench/research/run_evaluation_20240428_verify/testbed/gold/pytest/5.2/tmpsxg7_z0p/temp_pytest-dev__pytest-6214_pred_try.patch 
[pytest-dev__pytest__5.2] [pytest-dev__pytest-6214] Subprocess args: {"check": false, "shell": false, "capture_output": false, "text": true, "env": {"CONDA_PKGS_DIRS": "/home/user/SWE-bench/research/run_evaluation_20240428_verify/testbed/gold/pytest/5.2/tmpmb4ql99z/miniconda3/cache"}, "stdout": -1, "stderr": -2} 
[pytest-dev__pytest__5.2] [pytest-dev__pytest-6214] Std. Output:
Checking patch src/_pytest/setupplan.py...
Applied patch src/_pytest/setupplan.py cleanly.
 
[pytest-dev__pytest__5.2] [pytest-dev__pytest-6214] Return Code: 0 
[pytest-dev__pytest__5.2] [pytest-dev__pytest-6214] Revert patch successful (pred_try) 
>>>>> Applied Patch (pred_try)
[pytest-dev__pytest__5.2] [pytest-dev__pytest-6214] Running installation command: . /home/user/SWE-bench/research/run_evaluation_20240428_verify/testbed/gold/pytest/5.2/tmpmb4ql99z/miniconda3/bin/activate pytest-dev__pytest__5.2 && echo 'activate successful' && pip install -e . 
[pytest-dev__pytest__5.2] [pytest-dev__pytest-6214] Command: . /home/user/SWE-bench/research/run_evaluation_20240428_verify/testbed/gold/pytest/5.2/tmpmb4ql99z/miniconda3/bin/activate pytest-dev__pytest__5.2 && echo 'activate successful' && pip install -e . 
[pytest-dev__pytest__5.2] [pytest-dev__pytest-6214] Subprocess args: {"check": true, "shell": true, "capture_output": false, "text": true, "env": {"CONDA_PKGS_DIRS": "/home/user/SWE-bench/research/run_evaluation_20240428_verify/testbed/gold/pytest/5.2/tmpmb4ql99z/miniconda3/cache"}, "stdout": -1, "stderr": -2, "timeout": 900} 
[pytest-dev__pytest__5.2] [pytest-dev__pytest-6214] Std. Output:
activate successful
Obtaining file:///home/user/SWE-bench/research/run_evaluation_20240428_verify/testbed/gold/pytest/5.2/tmpsxg7_z0p/pytest-dev__pytest__5.2
  Installing build dependencies: started
  Installing build dependencies: finished with status 'done'
  Checking if build backend supports build_editable: started
  Checking if build backend supports build_editable: finished with status 'done'
  Getting requirements to build editable: started
  Getting requirements to build editable: finished with status 'done'
  Preparing editable metadata (pyproject.toml): started
  Preparing editable metadata (pyproject.toml): finished with status 'done'
Requirement already satisfied: py>=1.5.0 in /home/user/SWE-bench/research/run_evaluation_20240428_verify/testbed/gold/pytest/5.2/tmpmb4ql99z/miniconda3/lib/python3.9/site-packages (from pytest==5.2.5.dev4+gf24f20a46) (1.11.0)
Requirement already satisfied: packaging in /home/user/SWE-bench/research/run_evaluation_20240428_verify/testbed/gold/pytest/5.2/tmpmb4ql99z/miniconda3/lib/python3.9/site-packages (from pytest==5.2.5.dev4+gf24f20a46) (23.1)
Requirement already satisfied: attrs>=17.4.0 in /home/user/SWE-bench/research/run_evaluation_20240428_verify/testbed/gold/pytest/5.2/tmpmb4ql99z/miniconda3/lib/python3.9/site-packages (from pytest==5.2.5.dev4+gf24f20a46) (23.1.0)
Requirement already satisfied: more-itertools>=4.0.0 in /home/user/SWE-bench/research/run_evaluation_20240428_verify/testbed/gold/pytest/5.2/tmpmb4ql99z/miniconda3/lib/python3.9/site-packages (from pytest==5.2.5.dev4+gf24f20a46) (10.1.0)
Requirement already satisfied: atomicwrites>=1.0 in /home/user/SWE-bench/research/run_evaluation_20240428_verify/testbed/gold/pytest/5.2/tmpmb4ql99z/miniconda3/lib/python3.9/site-packages (from pytest==5.2.5.dev4+gf24f20a46) (1.4.1)
Requirement already satisfied: pluggy<1.0,>=0.12 in /home/user/SWE-bench/research/run_evaluation_20240428_verify/testbed/gold/pytest/5.2/tmpmb4ql99z/miniconda3/lib/python3.9/site-packages (from pytest==5.2.5.dev4+gf24f20a46) (0.13.1)
Requirement already satisfied: wcwidth in /home/user/SWE-bench/research/run_evaluation_20240428_verify/testbed/gold/pytest/5.2/tmpmb4ql99z/miniconda3/lib/python3.9/site-packages (from pytest==5.2.5.dev4+gf24f20a46) (0.2.6)
Building wheels for collected packages: pytest
  Building editable for pytest (pyproject.toml): started
  Building editable for pytest (pyproject.toml): finished with status 'done'
  Created wheel for pytest: filename=pytest-5.2.5.dev4+gf24f20a46-0.editable-py3-none-any.whl size=5069 sha256=2db8cbacb3f8df77ec954d7e4fa8936359aae215b4d9cabc098ab9573c0bb33d
  Stored in directory: /tmp/pip-ephem-wheel-cache-xl_4poam/wheels/47/d3/fe/74d99643380b4a0285fb278c1980a9d63d5a91db1744d11dbc
Successfully built pytest
Installing collected packages: pytest
Successfully installed pytest-5.2.5.dev4+gf24f20a46
 
[pytest-dev__pytest__5.2] [pytest-dev__pytest-6214] Return Code: 0 
[pytest-dev__pytest__5.2] [pytest-dev__pytest-6214] Installation successful 

>>>>> Init Succeeded
[pytest-dev__pytest__5.2] [pytest-dev__pytest-6214] Command: git apply -v /home/user/SWE-bench/research/run_evaluation_20240428_verify/testbed/gold/pytest/5.2/tmpsxg7_z0p/temp_pytest-dev__pytest-6214_pred.patch 
[pytest-dev__pytest__5.2] [pytest-dev__pytest-6214] Subprocess args: {"check": false, "shell": false, "capture_output": false, "text": true, "env": {"CONDA_PKGS_DIRS": "/home/user/SWE-bench/research/run_evaluation_20240428_verify/testbed/gold/pytest/5.2/tmpmb4ql99z/miniconda3/cache"}, "stdout": -1, "stderr": -2} 
[pytest-dev__pytest__5.2] [pytest-dev__pytest-6214] Std. Output:
Checking patch src/_pytest/setupplan.py...
Applied patch src/_pytest/setupplan.py cleanly.
 
[pytest-dev__pytest__5.2] [pytest-dev__pytest-6214] Return Code: 0 
[pytest-dev__pytest__5.2] [pytest-dev__pytest-6214] Apply patch successful (pred) 
>>>>> Applied Patch (pred)
[pytest-dev__pytest__5.2] [pytest-dev__pytest-6214] Command: git restore testing/python/setup_plan.py 
[pytest-dev__pytest__5.2] [pytest-dev__pytest-6214] Subprocess args: {"check": true, "shell": false, "capture_output": false, "text": true, "env": {"CONDA_PKGS_DIRS": "/home/user/SWE-bench/research/run_evaluation_20240428_verify/testbed/gold/pytest/5.2/tmpmb4ql99z/miniconda3/cache"}, "stdout": -1, "stderr": -2} 
[pytest-dev__pytest__5.2] [pytest-dev__pytest-6214] Std. Output:
 
[pytest-dev__pytest__5.2] [pytest-dev__pytest-6214] Return Code: 0 
[pytest-dev__pytest__5.2] [pytest-dev__pytest-6214] Command: git apply -v /home/user/SWE-bench/research/run_evaluation_20240428_verify/testbed/gold/pytest/5.2/tmpsxg7_z0p/temp_pytest-dev__pytest-6214_test.patch 
[pytest-dev__pytest__5.2] [pytest-dev__pytest-6214] Subprocess args: {"check": false, "shell": false, "capture_output": false, "text": true, "env": {"CONDA_PKGS_DIRS": "/home/user/SWE-bench/research/run_evaluation_20240428_verify/testbed/gold/pytest/5.2/tmpmb4ql99z/miniconda3/cache"}, "stdout": -1, "stderr": -2} 
[pytest-dev__pytest__5.2] [pytest-dev__pytest-6214] Std. Output:
Checking patch testing/python/setup_plan.py...
Applied patch testing/python/setup_plan.py cleanly.
 
[pytest-dev__pytest__5.2] [pytest-dev__pytest-6214] Return Code: 0 
[pytest-dev__pytest__5.2] [pytest-dev__pytest-6214] Apply patch successful (test) 
>>>>> Applied Patch (test)
Test Script: . /home/user/SWE-bench/research/run_evaluation_20240428_verify/testbed/gold/pytest/5.2/tmpmb4ql99z/miniconda3/bin/activate pytest-dev__pytest__5.2 && echo 'activate successful' && pytest -rA testing/python/setup_plan.py;
[pytest-dev__pytest__5.2] [pytest-dev__pytest-6214] Command: . /home/user/SWE-bench/research/run_evaluation_20240428_verify/testbed/gold/pytest/5.2/tmpmb4ql99z/miniconda3/bin/activate pytest-dev__pytest__5.2 && echo 'activate successful' && pytest -rA testing/python/setup_plan.py 
[pytest-dev__pytest__5.2] [pytest-dev__pytest-6214] Subprocess args: {"check": false, "shell": true, "capture_output": false, "text": true, "env": {"CONDA_PKGS_DIRS": "/home/user/SWE-bench/research/run_evaluation_20240428_verify/testbed/gold/pytest/5.2/tmpmb4ql99z/miniconda3/cache"}, "stdout": -1, "stderr": -2, "timeout": 900} 
[pytest-dev__pytest__5.2] [pytest-dev__pytest-6214] Std. Output:
activate successful
============================= test session starts ==============================
platform linux -- Python 3.9.18, pytest-5.2.5.dev4+gf24f20a46, py-1.11.0, pluggy-0.13.1
rootdir: /home/user/SWE-bench/research/run_evaluation_20240428_verify/testbed/gold/pytest/5.2/tmpsxg7_z0p/pytest-dev__pytest__5.2, inifile: tox.ini
collected 3 items

testing/python/setup_plan.py ...                                         [100%]

==================================== PASSES ====================================
_________________________ test_show_fixtures_and_test __________________________
----------------------------- Captured stdout call -----------------------------
============================= test session starts ==============================
platform linux -- Python 3.9.18, pytest-5.2.5.dev4+gf24f20a46, py-1.11.0, pluggy-0.13.1
rootdir: /tmp/pytest-of-user/pytest-17/test_show_fixtures_and_test0
collected 2 items

test1.yaml 
        test1.yaml::test1.yaml
test_show_fixtures_and_test.py 
        SETUP    F arg
        test_show_fixtures_and_test.py::test_arg (fixtures used: arg)
        TEARDOWN F arg

============================ no tests ran in 0.01s =============================
_______ test_show_multi_test_fixture_setup_and_teardown_correctly_simple _______
----------------------------- Captured stdout call -----------------------------
============================= test session starts ==============================
platform linux -- Python 3.9.18, pytest-5.2.5.dev4+gf24f20a46, py-1.11.0, pluggy-0.13.1
rootdir: /tmp/pytest-of-user/pytest-17/test_show_multi_test_fixture_setup_and_teardown_correctly_simple0
collected 2 items

test_show_multi_test_fixture_setup_and_teardown_correctly_simple.py 
      SETUP    C fix
        test_show_multi_test_fixture_setup_and_teardown_correctly_simple.py::TestClass::test_one (fixtures used: fix)
        test_show_multi_test_fixture_setup_and_teardown_correctly_simple.py::TestClass::test_two (fixtures used: fix)
      TEARDOWN C fix

============================ no tests ran in 0.01s =============================
______ test_show_multi_test_fixture_setup_and_teardown_same_as_setup_show ______
----------------------------- Captured stdout call -----------------------------
============================= test session starts ==============================
platform linux -- Python 3.9.18, pytest-5.2.5.dev4+gf24f20a46, py-1.11.0, pluggy-0.13.1
rootdir: /tmp/pytest-of-user/pytest-17/test_show_multi_test_fixture_setup_and_teardown_same_as_setup_show0
collected 3 items

test_show_multi_test_fixture_setup_and_teardown_same_as_setup_show.py 
SETUP    S sess
    SETUP    M mod
      SETUP    C cls
        SETUP    F func
        test_show_multi_test_fixture_setup_and_teardown_same_as_setup_show.py::test_outside (fixtures used: cls, func, mod, sess)
        TEARDOWN F func
      TEARDOWN C cls
      SETUP    C cls
        SETUP    F func
        test_show_multi_test_fixture_setup_and_teardown_same_as_setup_show.py::TestCls::test_one (fixtures used: cls, func, mod, sess)
        TEARDOWN F func
        SETUP    F func
        test_show_multi_test_fixture_setup_and_teardown_same_as_setup_show.py::TestCls::test_two (fixtures used: cls, func, mod, sess)
        TEARDOWN F func
      TEARDOWN C cls
    TEARDOWN M mod
TEARDOWN S sess

============================ no tests ran in 0.01s =============================
============================= test session starts ==============================
platform linux -- Python 3.9.18, pytest-5.2.5.dev4+gf24f20a46, py-1.11.0, pluggy-0.13.1
rootdir: /tmp/pytest-of-user/pytest-17/test_show_multi_test_fixture_setup_and_teardown_same_as_setup_show0
collected 3 items

test_show_multi_test_fixture_setup_and_teardown_same_as_setup_show.py 
SETUP    S sess
    SETUP    M mod
      SETUP    C cls
        SETUP    F func
        test_show_multi_test_fixture_setup_and_teardown_same_as_setup_show.py::test_outside (fixtures used: cls, func, mod, sess).
        TEARDOWN F func
      TEARDOWN C cls
      SETUP    C cls
        SETUP    F func
        test_show_multi_test_fixture_setup_and_teardown_same_as_setup_show.py::TestCls::test_one (fixtures used: cls, func, mod, sess).
        TEARDOWN F func
        SETUP    F func
        test_show_multi_test_fixture_setup_and_teardown_same_as_setup_show.py::TestCls::test_two (fixtures used: cls, func, mod, sess).
        TEARDOWN F func
      TEARDOWN C cls
    TEARDOWN M mod
TEARDOWN S sess

============================== 3 passed in 0.01s ===============================
=========================== short test summary info ============================
PASSED testing/python/setup_plan.py::test_show_fixtures_and_test
PASSED testing/python/setup_plan.py::test_show_multi_test_fixture_setup_and_teardown_correctly_simple
PASSED testing/python/setup_plan.py::test_show_multi_test_fixture_setup_and_teardown_same_as_setup_show
============================== 3 passed in 0.20s ===============================
 
[pytest-dev__pytest__5.2] [pytest-dev__pytest-6214] Return Code: 0 

>>>>> All Tests Passed
[pytest-dev__pytest__5.2] [pytest-dev__pytest-6214] Test script run successful 
